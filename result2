----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 128, 63, 63]           1,152
              ReLU-2          [-1, 128, 63, 63]               0
         MaxPool2d-3          [-1, 128, 31, 31]               0
           Flatten-4             [-1, 128, 961]               0
         Tokenizer-5             [-1, 961, 128]               0
           Dropout-6             [-1, 961, 128]               0
         LayerNorm-7             [-1, 961, 128]             256
            Linear-8             [-1, 961, 384]          49,152
           Dropout-9          [-1, 4, 961, 961]               0
           Linear-10             [-1, 961, 128]          16,512
          Dropout-11             [-1, 961, 128]               0
        Attention-12             [-1, 961, 128]               0
         Identity-13             [-1, 961, 128]               0
        LayerNorm-14             [-1, 961, 128]             256
           Linear-15             [-1, 961, 512]          66,048
          Dropout-16             [-1, 961, 512]               0
           Linear-17             [-1, 961, 128]          65,664
          Dropout-18             [-1, 961, 128]               0
         Identity-19             [-1, 961, 128]               0
TransformerEncoderLayer-20             [-1, 961, 128]               0
        LayerNorm-21             [-1, 961, 128]             256
           Linear-22             [-1, 961, 384]          49,152
          Dropout-23          [-1, 4, 961, 961]               0
           Linear-24             [-1, 961, 128]          16,512
          Dropout-25             [-1, 961, 128]               0
        Attention-26             [-1, 961, 128]               0
         DropPath-27             [-1, 961, 128]               0
        LayerNorm-28             [-1, 961, 128]             256
           Linear-29             [-1, 961, 512]          66,048
          Dropout-30             [-1, 961, 512]               0
           Linear-31             [-1, 961, 128]          65,664
          Dropout-32             [-1, 961, 128]               0
         DropPath-33             [-1, 961, 128]               0
TransformerEncoderLayer-34             [-1, 961, 128]               0
        LayerNorm-35             [-1, 961, 128]             256
           Linear-36             [-1, 961, 384]          49,152
          Dropout-37          [-1, 4, 961, 961]               0
           Linear-38             [-1, 961, 128]          16,512
          Dropout-39             [-1, 961, 128]               0
        Attention-40             [-1, 961, 128]               0
         DropPath-41             [-1, 961, 128]               0
        LayerNorm-42             [-1, 961, 128]             256
           Linear-43             [-1, 961, 512]          66,048
          Dropout-44             [-1, 961, 512]               0
           Linear-45             [-1, 961, 128]          65,664
          Dropout-46             [-1, 961, 128]               0
         DropPath-47             [-1, 961, 128]               0
TransformerEncoderLayer-48             [-1, 961, 128]               0
        LayerNorm-49             [-1, 961, 128]             256
           Linear-50             [-1, 961, 384]          49,152
          Dropout-51          [-1, 4, 961, 961]               0
           Linear-52             [-1, 961, 128]          16,512
          Dropout-53             [-1, 961, 128]               0
        Attention-54             [-1, 961, 128]               0
         DropPath-55             [-1, 961, 128]               0
        LayerNorm-56             [-1, 961, 128]             256
           Linear-57             [-1, 961, 512]          66,048
          Dropout-58             [-1, 961, 512]               0
           Linear-59             [-1, 961, 128]          65,664
          Dropout-60             [-1, 961, 128]               0
         DropPath-61             [-1, 961, 128]               0
TransformerEncoderLayer-62             [-1, 961, 128]               0
        LayerNorm-63             [-1, 961, 128]             256
           Linear-64               [-1, 961, 1]             129
           Linear-65                    [-1, 2]             258
TransformerClassifier-66                    [-1, 2]               0
================================================================
Total params: 793,347
Trainable params: 793,347
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.06
Forward/backward pass size (MB): 204.02
Params size (MB): 3.03
Estimated Total Size (MB): 207.11
----------------------------------------------------------------




{'best_acc': 85.09615384615384, 'test_accs': [63.94230769230769, 68.75, 70.83333333333333, 69.87179487179488, 71.47435897435898, 70.99358974358974, 70.99358974358974, 72.59615384615384, 71.9551282051282, 74.51923076923077, 77.88461538461539, 77.88461538461539, 80.28846153846153, 80.44871794871794, 75.32051282051282, 76.12179487179488, 80.28846153846153, 81.41025641025641, 79.6474358974359, 81.25, 81.73076923076923, 80.76923076923077, 83.01282051282051, 84.2948717948718, 81.57051282051282, 83.33333333333333, 79.16666666666667, 81.73076923076923, 81.25, 81.41025641025641, 80.28846153846153, 80.76923076923077, 80.92948717948718, 79.48717948717949, 79.6474358974359, 85.09615384615384, 80.44871794871794, 76.92307692307692, 80.28846153846153, 77.40384615384616, 79.32692307692308, 78.52564102564102, 79.16666666666667, 78.68589743589743, 81.41025641025641, 79.00641025641026, 78.52564102564102, 75.96153846153847, 77.88461538461539, 83.49358974358974, 75.80128205128206, 81.08974358974359, 77.72435897435898, 77.72435897435898, 78.0448717948718, 81.57051282051282, 81.08974358974359, 81.89102564102564, 81.41025641025641, 73.5576923076923, 82.53205128205128, 78.68589743589743, 79.16666666666667, 83.49358974358974, 81.41025641025641, 77.08333333333333, 80.12820512820512, 74.83974358974359, 82.53205128205128, 78.52564102564102, 77.56410256410257, 83.17307692307692, 81.41025641025641, 75.80128205128206, 80.60897435897436, 80.28846153846153, 81.89102564102564, 79.8076923076923, 75.96153846153847, 76.92307692307692, 73.5576923076923, 76.92307692307692, 80.44871794871794, 80.12820512820512, 80.60897435897436, 82.53205128205128, 74.51923076923077, 77.08333333333333, 75.16025641025641, 82.8525641025641, 71.47435897435898, 76.28205128205128, 82.8525641025641, 78.52564102564102, 76.6025641025641, 75.48076923076923, 77.24358974358974, 81.25, 78.0448717948718, 79.8076923076923], 'test_losses': [0.8006900671200875, 0.776114858686924, 0.6691396138989009, 0.6221416401557434, 0.6169950308708044, 0.7108762049331114, 0.7557965873334652, 0.6081268974603751, 0.6707467388075131, 0.6378575618832539, 0.5658201036544946, 0.6567179104790856, 0.4979497035726523, 0.442053232055444, 0.7642937201337937, 0.7180868684290311, 0.4582307691184374, 0.4302365343349102, 0.5826984485850121, 0.44407233194662976, 0.5264056937243694, 0.5155915127326853, 0.4331684197084262, 0.4137709907805308, 0.5647622862806878, 0.4772036826381317, 0.6563416465949745, 0.578245701853377, 0.617942365208784, 0.5689987663824397, 0.76275816354423, 0.6486441055969454, 0.6942176172294869, 0.641371102251399, 0.718537012888238, 0.43307457035646224, 0.6408271294039412, 0.7793308842318276, 0.5798832457870818, 0.8317682943277013, 0.7345768512787823, 0.682932281627869, 0.709192617987402, 0.6402105815971318, 0.5179599716256444, 0.6241822658130565, 0.9184755033273131, 1.0135729419939123, 0.6304950566174319, 0.530284084021472, 0.7666703273291484, 0.6153327126461917, 0.7419793558760713, 0.8615252557061374, 0.8268360102740235, 0.6963175616871852, 0.6274366116222854, 0.5309520562251027, 0.5976655118597242, 0.7989416485174726, 0.5902664755733732, 0.797550641782343, 0.5710139350058177, 0.5599337471171449, 0.5724087554531602, 0.832254508862463, 0.7447204852405076, 0.9012112099235543, 0.6285777449822769, 0.7292504385054016, 0.845843664627188, 0.48276036447630477, 0.7216353650169017, 0.9792512640810739, 0.6892662514836934, 0.7273113910908787, 0.5547028588704191, 0.6517262938611496, 1.0408647838735188, 0.888942755979653, 1.0104837354186038, 0.7539357968832915, 0.5848433871466953, 0.7493904760071578, 0.688399521048921, 0.4547891868517185, 0.902968110743528, 0.8698697831654826, 0.8312593382836965, 0.5899612582169282, 1.243237606084828, 0.7676420142301, 0.5901205680953959, 0.8194733754513809, 0.8425469654851044, 0.9751566827882272, 0.7767521531428568, 0.6532330066443254, 0.9271361417949009, 0.7523263175762855], 'train_acss': [75.11503067484662, 82.20858895705521, 83.72315950920246, 84.20245398773007, 85.04601226993866, 85.75536809815951, 85.21855828220859, 85.58282208588957, 86.94401840490798, 87.4808282208589, 87.74923312883436, 88.26687116564418, 88.65030674846626, 88.8611963190184, 89.2638036809816, 89.83895705521472, 89.7430981595092, 90.06901840490798, 89.95398773006134, 90.47162576687117, 90.87423312883436, 90.72085889570552, 90.75920245398773, 91.0659509202454, 91.35352760736197, 90.85506134969326, 91.0467791411043, 91.33435582822086, 91.0851226993865, 91.64110429447852, 91.67944785276073, 91.46855828220859, 92.06288343558282, 91.9286809815951, 91.69861963190183, 92.29294478527608, 92.15874233128834, 92.33128834355828, 92.04371165644172, 92.75306748466258, 92.4271472392638, 92.17791411042944, 92.61886503067484, 92.50383435582822, 92.35046012269939, 92.48466257668711, 92.8489263803681, 92.96395705521472, 93.00230061349693, 93.04064417177914, 93.04064417177914, 93.09815950920246, 93.13650306748467, 93.25153374233129, 93.59662576687117, 93.2898773006135, 93.34739263803681, 93.2707055214724, 93.46242331288343, 93.34739263803681, 93.3282208588957, 93.36656441717791, 93.36656441717791, 93.55828220858896, 93.50076687116564, 93.57745398773007, 93.67331288343559, 94.01840490797547, 93.40490797546012, 94.15260736196319, 93.80751533742331, 94.1717791411043, 93.7691717791411, 94.03757668711657, 93.75, 93.90337423312883, 94.45935582822086, 94.1717791411043, 94.44018404907976, 94.15260736196319, 94.32515337423312, 94.30598159509202, 94.09509202453988, 94.38266871165644, 94.01840490797547, 94.26763803680981, 94.84279141104294, 94.51687116564418, 94.53604294478528, 94.07592024539878, 94.49769938650307, 94.51687116564418, 94.42101226993866, 94.47852760736197, 94.44018404907976, 94.74693251533742, 94.44018404907976, 94.6127300613497, 94.670245398773, 94.97699386503068], 'train_losses': [0.5043644517934396, 0.42402798385342205, 0.38588243121841204, 0.3615378766360641, 0.3505325809669641, 0.3370489173460226, 0.33638617762027345, 0.3260496941629363, 0.30771197370636316, 0.2979303128743099, 0.2839589037709624, 0.27551702430551767, 0.2688784896527514, 0.25418902593064896, 0.240809443103938, 0.2451179936196815, 0.239536253815414, 0.2316797423387872, 0.23159379046410322, 0.22074428910355262, 0.22315099554856313, 0.2205706885301628, 0.2182380542776153, 0.21537238064627706, 0.21081084652912396, 0.21019356411830323, 0.21488759723817644, 0.20953442155172122, 0.20854606499975445, 0.2002400807476391, 0.19956651382559648, 0.19987240095445716, 0.1967837785776316, 0.19890434794882142, 0.19680906523110495, 0.18928427316042726, 0.1895410560751391, 0.18293200964622336, 0.19081609359045335, 0.18180338779544356, 0.18205735227664463, 0.1865707160162231, 0.17555284980067445, 0.1839043985469484, 0.18246441398478724, 0.17989334288664574, 0.1730991868585051, 0.17222524425043423, 0.1749404507626328, 0.17461073752835485, 0.17221964026966985, 0.17369376250079896, 0.16581918804783466, 0.163942497829261, 0.16753302651966, 0.1662969233702434, 0.16838125569711618, 0.16461925336696848, 0.163947081434729, 0.1638859736853125, 0.1630895160498566, 0.1623872464402733, 0.15944409621250957, 0.1606949489946517, 0.16146559843313565, 0.16653998730349942, 0.1573901651455718, 0.15201097831524268, 0.15606982281511547, 0.14760405687969522, 0.15528418385977943, 0.14895718521867063, 0.15688215301263553, 0.14690896179641316, 0.15686162495185718, 0.1548197519354111, 0.14725318263667675, 0.1478265171264944, 0.14372724379795643, 0.1449137941957023, 0.14619603761660357, 0.1427600506173184, 0.14723839737514913, 0.1482146294570408, 0.14802926188860477, 0.14912467711873673, 0.13890287782525518, 0.14304867255547157, 0.14059536446148724, 0.14781766962202872, 0.13945783601868098, 0.14259358507615336, 0.1400739121082256, 0.142381397931568, 0.14065756582786426, 0.13720937370768307, 0.1381918264315835, 0.1324956597163245, 0.1373884486337335, 0.13368907740891003]}
2021-12-31 21:28:50.697369



model = CCT(
        img_size=128,
        embedding_dim=128,
        n_input_channels=1,
        n_conv_layers=1,
        kernel_size=3,
        stride=2,
        padding=0,
        pooling_kernel_size=3,
        pooling_stride=2,
        pooling_padding=0,
        num_layers=4,
        num_heads=4,
        mlp_radio=1.,
        num_classes=2,
        positional_embedding='None', # ['sine', 'learnable', 'none']
        )
print(summary(model.to(device), (1,128 ,128)))



----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 256, 64, 64]           2,304
              ReLU-2          [-1, 256, 64, 64]               0
         MaxPool2d-3          [-1, 256, 32, 32]               0
           Flatten-4            [-1, 256, 1024]               0
         Tokenizer-5            [-1, 1024, 256]               0
           Dropout-6            [-1, 1024, 256]               0
         LayerNorm-7            [-1, 1024, 256]             512
            Linear-8            [-1, 1024, 768]         196,608
           Dropout-9        [-1, 4, 1024, 1024]               0
           Linear-10            [-1, 1024, 256]          65,792
          Dropout-11            [-1, 1024, 256]               0
        Attention-12            [-1, 1024, 256]               0
         Identity-13            [-1, 1024, 256]               0
        LayerNorm-14            [-1, 1024, 256]             512
           Linear-15           [-1, 1024, 1024]         263,168
          Dropout-16           [-1, 1024, 1024]               0
           Linear-17            [-1, 1024, 256]         262,400
          Dropout-18            [-1, 1024, 256]               0
         Identity-19            [-1, 1024, 256]               0
TransformerEncoderLayer-20            [-1, 1024, 256]               0
        LayerNorm-21            [-1, 1024, 256]             512
           Linear-22            [-1, 1024, 768]         196,608
          Dropout-23        [-1, 4, 1024, 1024]               0
           Linear-24            [-1, 1024, 256]          65,792
          Dropout-25            [-1, 1024, 256]               0
        Attention-26            [-1, 1024, 256]               0
         DropPath-27            [-1, 1024, 256]               0
        LayerNorm-28            [-1, 1024, 256]             512
           Linear-29           [-1, 1024, 1024]         263,168
          Dropout-30           [-1, 1024, 1024]               0
           Linear-31            [-1, 1024, 256]         262,400
          Dropout-32            [-1, 1024, 256]               0
         DropPath-33            [-1, 1024, 256]               0
TransformerEncoderLayer-34            [-1, 1024, 256]               0
        LayerNorm-35            [-1, 1024, 256]             512
           Linear-36            [-1, 1024, 768]         196,608
          Dropout-37        [-1, 4, 1024, 1024]               0
           Linear-38            [-1, 1024, 256]          65,792
          Dropout-39            [-1, 1024, 256]               0
        Attention-40            [-1, 1024, 256]               0
         DropPath-41            [-1, 1024, 256]               0
        LayerNorm-42            [-1, 1024, 256]             512
           Linear-43           [-1, 1024, 1024]         263,168
          Dropout-44           [-1, 1024, 1024]               0
           Linear-45            [-1, 1024, 256]         262,400
          Dropout-46            [-1, 1024, 256]               0
         DropPath-47            [-1, 1024, 256]               0
TransformerEncoderLayer-48            [-1, 1024, 256]               0
        LayerNorm-49            [-1, 1024, 256]             512
           Linear-50            [-1, 1024, 768]         196,608
          Dropout-51        [-1, 4, 1024, 1024]               0
           Linear-52            [-1, 1024, 256]          65,792
          Dropout-53            [-1, 1024, 256]               0
        Attention-54            [-1, 1024, 256]               0
         DropPath-55            [-1, 1024, 256]               0
        LayerNorm-56            [-1, 1024, 256]             512
           Linear-57           [-1, 1024, 1024]         263,168
          Dropout-58           [-1, 1024, 1024]               0
           Linear-59            [-1, 1024, 256]         262,400
          Dropout-60            [-1, 1024, 256]               0
         DropPath-61            [-1, 1024, 256]               0
TransformerEncoderLayer-62            [-1, 1024, 256]               0
        LayerNorm-63            [-1, 1024, 256]             512
           Linear-64            [-1, 1024, 768]         196,608
          Dropout-65        [-1, 4, 1024, 1024]               0
           Linear-66            [-1, 1024, 256]          65,792
          Dropout-67            [-1, 1024, 256]               0
        Attention-68            [-1, 1024, 256]               0
         DropPath-69            [-1, 1024, 256]               0
        LayerNorm-70            [-1, 1024, 256]             512
           Linear-71           [-1, 1024, 1024]         263,168
          Dropout-72           [-1, 1024, 1024]               0
           Linear-73            [-1, 1024, 256]         262,400
          Dropout-74            [-1, 1024, 256]               0
         DropPath-75            [-1, 1024, 256]               0
TransformerEncoderLayer-76            [-1, 1024, 256]               0
        LayerNorm-77            [-1, 1024, 256]             512
           Linear-78            [-1, 1024, 768]         196,608
          Dropout-79        [-1, 4, 1024, 1024]               0
           Linear-80            [-1, 1024, 256]          65,792
          Dropout-81            [-1, 1024, 256]               0
        Attention-82            [-1, 1024, 256]               0
         DropPath-83            [-1, 1024, 256]               0
        LayerNorm-84            [-1, 1024, 256]             512
           Linear-85           [-1, 1024, 1024]         263,168
          Dropout-86           [-1, 1024, 1024]               0
           Linear-87            [-1, 1024, 256]         262,400
          Dropout-88            [-1, 1024, 256]               0
         DropPath-89            [-1, 1024, 256]               0
TransformerEncoderLayer-90            [-1, 1024, 256]               0
        LayerNorm-91            [-1, 1024, 256]             512
           Linear-92              [-1, 1024, 1]             257
           Linear-93                    [-1, 2]             514
TransformerClassifier-94                    [-1, 2]               0
================================================================
Total params: 4,737,539
Trainable params: 4,737,539
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.06
Forward/backward pass size (MB): 470.01
Params size (MB): 18.07
Estimated Total Size (MB): 488.14
----------------------------------------------------------------
None
2021-12-31 22:06:18.273237
Epoch: [0]	 Loss 0.3291 (0.4146)	 acc 75.000 (80.406)
Test	  accuracy: 77.404 (Err: 0.605 )

Epoch: [1]	 Loss 0.6062 (0.3392)	 acc 75.000 (85.008)
Test	  accuracy: 76.122 (Err: 0.620 )

Epoch: [2]	 Loss 0.0456 (0.3272)	 acc 100.000 (85.372)
Test	  accuracy: 76.603 (Err: 0.685 )

Epoch: [3]	 Loss 0.4979 (0.3222)	 acc 75.000 (85.257)
Test	  accuracy: 81.090 (Err: 0.520 )

Epoch: [4]	 Loss 0.3682 (0.3134)	 acc 75.000 (86.350)
Test	  accuracy: 78.045 (Err: 0.529 )

Epoch: [5]	 Loss 0.1636 (0.3068)	 acc 100.000 (86.618)
Test	  accuracy: 72.596 (Err: 0.582 )

Epoch: [6]	 Loss 0.1954 (0.2955)	 acc 100.000 (87.270)
Test	  accuracy: 77.083 (Err: 0.508 )

Epoch: [7]	 Loss 0.2995 (0.2949)	 acc 87.500 (86.733)
Test	  accuracy: 71.154 (Err: 0.847 )

Epoch: [8]	 Loss 0.0465 (0.2879)	 acc 100.000 (86.925)
Test	  accuracy: 77.724 (Err: 0.562 )

Epoch: [9]	 Loss 0.1213 (0.2833)	 acc 100.000 (87.481)
Test	  accuracy: 71.635 (Err: 0.714 )

Epoch: [10]	 Loss 0.4518 (0.2799)	 acc 75.000 (87.289)
Test	  accuracy: 76.923 (Err: 0.611 )

Epoch: [11]	 Loss 0.2017 (0.2714)	 acc 100.000 (87.960)
Test	  accuracy: 74.519 (Err: 0.768 )

Epoch: [12]	 Loss 0.1995 (0.2678)	 acc 87.500 (88.075)
Test	  accuracy: 77.564 (Err: 0.679 )

Epoch: [13]	 Loss 0.0435 (0.2632)	 acc 100.000 (88.516)
Test	  accuracy: 78.686 (Err: 0.648 )

Epoch: [14]	 Loss 0.0378 (0.2540)	 acc 100.000 (88.516)
Test	  accuracy: 78.205 (Err: 0.615 )

Epoch: [15]	 Loss 0.4726 (0.2493)	 acc 87.500 (88.842)
Test	  accuracy: 77.564 (Err: 0.703 )

Epoch: [16]	 Loss 0.1518 (0.2404)	 acc 100.000 (89.417)
Test	  accuracy: 79.808 (Err: 0.530 )

Epoch: [17]	 Loss 0.3574 (0.2265)	 acc 75.000 (90.069)
Test	  accuracy: 79.968 (Err: 0.665 )

Epoch: [18]	 Loss 0.1569 (0.2242)	 acc 100.000 (90.146)
Test	  accuracy: 75.481 (Err: 0.861 )

Epoch: [19]	 Loss 0.4410 (0.2164)	 acc 75.000 (90.280)
Test	  accuracy: 81.410 (Err: 0.572 )

Epoch: [20]	 Loss 0.0654 (0.2108)	 acc 100.000 (90.146)
Test	  accuracy: 76.763 (Err: 0.705 )

Epoch: [21]	 Loss 0.0181 (0.2070)	 acc 100.000 (91.123)
Test	  accuracy: 78.686 (Err: 0.639 )

Epoch: [22]	 Loss 0.3036 (0.2050)	 acc 75.000 (91.200)
Test	  accuracy: 79.167 (Err: 0.622 )

Epoch: [23]	 Loss 0.2601 (0.1994)	 acc 87.500 (91.545)
Test	  accuracy: 79.647 (Err: 0.674 )

Epoch: [24]	 Loss 0.0306 (0.1942)	 acc 100.000 (91.737)
Test	  accuracy: 71.955 (Err: 0.874 )

Epoch: [25]	 Loss 0.1289 (0.1963)	 acc 100.000 (92.044)
Test	  accuracy: 76.442 (Err: 0.723 )

Epoch: [26]	 Loss 0.1429 (0.1915)	 acc 100.000 (92.005)
Test	  accuracy: 80.769 (Err: 0.493 )

Epoch: [27]	 Loss 0.0876 (0.1862)	 acc 100.000 (92.025)
Test	  accuracy: 81.410 (Err: 0.593 )

Epoch: [28]	 Loss 0.2725 (0.1867)	 acc 87.500 (92.159)
Test	  accuracy: 74.519 (Err: 0.929 )

Epoch: [29]	 Loss 0.1553 (0.1850)	 acc 87.500 (92.120)
Test	  accuracy: 79.487 (Err: 0.690 )

Epoch: [30]	 Loss 0.1499 (0.1829)	 acc 87.500 (92.293)
Test	  accuracy: 77.564 (Err: 0.758 )

Epoch: [31]	 Loss 0.1390 (0.1824)	 acc 100.000 (92.235)
Test	  accuracy: 71.795 (Err: 1.107 )

Epoch: [32]	 Loss 0.0244 (0.1781)	 acc 100.000 (92.542)
Test	  accuracy: 75.801 (Err: 0.771 )

Epoch: [33]	 Loss 0.2655 (0.1795)	 acc 87.500 (92.504)
Test	  accuracy: 81.891 (Err: 0.559 )

Epoch: [34]	 Loss 0.1728 (0.1762)	 acc 87.500 (93.041)
Test	  accuracy: 80.929 (Err: 0.615 )

Epoch: [35]	 Loss 0.0227 (0.1717)	 acc 100.000 (92.887)
Test	  accuracy: 80.449 (Err: 0.553 )

Epoch: [36]	 Loss 0.0027 (0.1691)	 acc 100.000 (92.887)
Test	  accuracy: 80.769 (Err: 0.502 )

Epoch: [37]	 Loss 0.2709 (0.1689)	 acc 87.500 (93.328)
Test	  accuracy: 82.853 (Err: 0.514 )

Epoch: [38]	 Loss 0.6625 (0.1681)	 acc 75.000 (93.002)
Test	  accuracy: 75.481 (Err: 0.854 )

Epoch: [39]	 Loss 0.1189 (0.1626)	 acc 100.000 (92.983)
Test	  accuracy: 77.244 (Err: 0.811 )

Epoch: [40]	 Loss 0.0793 (0.1609)	 acc 100.000 (93.750)
Test	  accuracy: 79.006 (Err: 0.665 )

Epoch: [41]	 Loss 0.3713 (0.1603)	 acc 75.000 (93.290)
Test	  accuracy: 84.295 (Err: 0.528 )

Epoch: [42]	 Loss 0.1864 (0.1657)	 acc 87.500 (93.367)
Test	  accuracy: 76.282 (Err: 0.858 )

Epoch: [43]	 Loss 0.2629 (0.1619)	 acc 87.500 (93.386)
Test	  accuracy: 82.051 (Err: 0.521 )

Epoch: [44]	 Loss 0.1745 (0.1565)	 acc 87.500 (93.520)
Test	  accuracy: 81.891 (Err: 0.591 )

Epoch: [45]	 Loss 0.0046 (0.1494)	 acc 100.000 (94.306)
Test	  accuracy: 82.372 (Err: 0.706 )

Epoch: [46]	 Loss 0.1069 (0.1550)	 acc 100.000 (93.462)
Test	  accuracy: 80.609 (Err: 0.680 )

Epoch: [47]	 Loss 0.0193 (0.1499)	 acc 100.000 (93.788)
Test	  accuracy: 77.244 (Err: 0.835 )

Epoch: [48]	 Loss 0.3548 (0.1572)	 acc 87.500 (93.558)
Test	  accuracy: 79.167 (Err: 0.832 )

Epoch: [49]	 Loss 0.3388 (0.1469)	 acc 87.500 (93.961)
Test	  accuracy: 80.128 (Err: 0.708 )

Epoch: [50]	 Loss 0.0276 (0.1497)	 acc 100.000 (93.597)
Test	  accuracy: 73.397 (Err: 1.017 )

Epoch: [51]	 Loss 0.0249 (0.1534)	 acc 100.000 (93.712)
Test	  accuracy: 80.929 (Err: 0.629 )

Epoch: [52]	 Loss 0.3553 (0.1508)	 acc 87.500 (93.846)
Test	  accuracy: 78.205 (Err: 0.769 )

Epoch: [53]	 Loss 0.1904 (0.1477)	 acc 87.500 (94.133)
Test	  accuracy: 76.603 (Err: 0.952 )

Epoch: [54]	 Loss 0.0378 (0.1461)	 acc 100.000 (94.268)
Test	  accuracy: 75.321 (Err: 1.018 )

Epoch: [55]	 Loss 0.2598 (0.1424)	 acc 87.500 (94.613)
Test	  accuracy: 77.404 (Err: 0.772 )

Epoch: [56]	 Loss 0.0134 (0.1446)	 acc 100.000 (94.421)
Test	  accuracy: 81.571 (Err: 0.647 )

Epoch: [57]	 Loss 0.0195 (0.1477)	 acc 100.000 (94.191)
Test	  accuracy: 81.571 (Err: 0.576 )

Epoch: [58]	 Loss 0.0396 (0.1434)	 acc 100.000 (94.344)
Test	  accuracy: 81.571 (Err: 0.545 )

Epoch: [59]	 Loss 0.2558 (0.1404)	 acc 87.500 (94.153)
Test	  accuracy: 80.449 (Err: 0.761 )

Epoch: [60]	 Loss 0.0727 (0.1375)	 acc 100.000 (94.670)
Test	  accuracy: 83.173 (Err: 0.557 )

Epoch: [61]	 Loss 0.0378 (0.1420)	 acc 100.000 (94.210)
Test	  accuracy: 82.853 (Err: 0.566 )

Epoch: [62]	 Loss 0.0448 (0.1325)	 acc 100.000 (94.632)
Test	  accuracy: 79.487 (Err: 0.795 )

Epoch: [63]	 Loss 0.0526 (0.1389)	 acc 100.000 (94.728)
Test	  accuracy: 79.006 (Err: 0.761 )

Epoch: [64]	 Loss 0.0733 (0.1393)	 acc 100.000 (94.498)
Test	  accuracy: 76.603 (Err: 0.833 )

Epoch: [65]	 Loss 0.0389 (0.1309)	 acc 100.000 (94.574)
Test	  accuracy: 78.045 (Err: 0.844 )

Epoch: [66]	 Loss 0.0461 (0.1290)	 acc 100.000 (94.517)
Test	  accuracy: 78.205 (Err: 0.772 )

Epoch: [67]	 Loss 0.0099 (0.1311)	 acc 100.000 (94.536)
Test	  accuracy: 76.603 (Err: 0.941 )

Epoch: [68]	 Loss 0.0160 (0.1346)	 acc 100.000 (94.689)
Test	  accuracy: 79.167 (Err: 0.724 )

Epoch: [69]	 Loss 0.1020 (0.1321)	 acc 100.000 (94.498)
Test	  accuracy: 76.603 (Err: 0.861 )

Epoch: [70]	 Loss 0.0345 (0.1214)	 acc 100.000 (95.399)
Test	  accuracy: 75.801 (Err: 1.053 )

Epoch: [71]	 Loss 0.1958 (0.1319)	 acc 87.500 (94.900)
Test	  accuracy: 75.641 (Err: 1.029 )

Epoch: [72]	 Loss 0.0958 (0.1290)	 acc 100.000 (94.689)
Test	  accuracy: 79.327 (Err: 0.768 )

Epoch: [73]	 Loss 0.2669 (0.1232)	 acc 87.500 (94.881)
Test	  accuracy: 79.808 (Err: 0.682 )

Epoch: [74]	 Loss 0.4089 (0.1245)	 acc 87.500 (95.092)
Test	  accuracy: 84.936 (Err: 0.505 )

Epoch: [75]	 Loss 0.1121 (0.1262)	 acc 100.000 (94.632)
Test	  accuracy: 80.288 (Err: 0.770 )

Epoch: [76]	 Loss 0.2987 (0.1253)	 acc 87.500 (95.169)
Test	  accuracy: 79.968 (Err: 0.699 )

Epoch: [77]	 Loss 0.0364 (0.1269)	 acc 100.000 (94.900)
Test	  accuracy: 75.962 (Err: 1.079 )

Epoch: [78]	 Loss 0.0430 (0.1225)	 acc 100.000 (95.360)
Test	  accuracy: 82.532 (Err: 0.658 )

Epoch: [79]	 Loss 0.1138 (0.1204)	 acc 100.000 (95.322)
Test	  accuracy: 79.487 (Err: 0.744 )

Epoch: [80]	 Loss 0.0018 (0.1224)	 acc 100.000 (95.054)
Test	  accuracy: 72.115 (Err: 1.361 )

Epoch: [81]	 Loss 0.0397 (0.1194)	 acc 100.000 (95.284)
Test	  accuracy: 80.769 (Err: 0.692 )

Epoch: [82]	 Loss 0.1735 (0.1174)	 acc 87.500 (95.437)
Test	  accuracy: 76.442 (Err: 1.069 )

Epoch: [83]	 Loss 0.0342 (0.1218)	 acc 100.000 (94.958)
Test	  accuracy: 80.769 (Err: 0.612 )

Epoch: [84]	 Loss 0.0095 (0.1155)	 acc 100.000 (95.322)
Test	  accuracy: 71.795 (Err: 1.120 )

Epoch: [85]	 Loss 0.0787 (0.1133)	 acc 100.000 (95.475)
Test	  accuracy: 78.846 (Err: 0.920 )

Epoch: [86]	 Loss 0.0544 (0.1110)	 acc 100.000 (95.590)
Test	  accuracy: 74.199 (Err: 1.220 )

Epoch: [87]	 Loss 0.2138 (0.1149)	 acc 87.500 (95.514)
Test	  accuracy: 76.122 (Err: 1.002 )

Epoch: [88]	 Loss 0.0106 (0.1171)	 acc 100.000 (95.418)
Test	  accuracy: 80.609 (Err: 0.665 )

Epoch: [89]	 Loss 0.0008 (0.1146)	 acc 100.000 (95.399)
Test	  accuracy: 71.955 (Err: 1.231 )

Epoch: [90]	 Loss 0.2286 (0.1172)	 acc 87.500 (94.977)
Test	  accuracy: 75.481 (Err: 1.088 )

Epoch: [91]	 Loss 0.0916 (0.1125)	 acc 100.000 (95.648)
Test	  accuracy: 75.160 (Err: 1.103 )

Epoch: [92]	 Loss 0.0383 (0.1081)	 acc 100.000 (95.590)
Test	  accuracy: 67.949 (Err: 1.636 )

Epoch: [93]	 Loss 0.1710 (0.1119)	 acc 87.500 (95.648)
Test	  accuracy: 75.641 (Err: 1.098 )

Epoch: [94]	 Loss 0.0223 (0.1099)	 acc 100.000 (95.418)
Test	  accuracy: 80.128 (Err: 0.784 )

Epoch: [95]	 Loss 0.1290 (0.1171)	 acc 87.500 (95.341)
Test	  accuracy: 74.199 (Err: 1.010 )

Epoch: [96]	 Loss 0.0082 (0.1066)	 acc 100.000 (95.571)
Test	  accuracy: 78.686 (Err: 0.786 )

Epoch: [97]	 Loss 0.0010 (0.1091)	 acc 100.000 (95.821)
Test	  accuracy: 77.244 (Err: 0.996 )

Epoch: [98]	 Loss 0.0044 (0.1060)	 acc 100.000 (95.744)
Test	  accuracy: 75.962 (Err: 1.085 )

Epoch: [99]	 Loss 0.0570 (0.1058)	 acc 100.000 (95.629)
Test	  accuracy: 72.917 (Err: 1.260 )

{'best_acc': 84.93589743589743, 'test_accs': [77.40384615384616, 76.12179487179488, 76.6025641025641, 81.08974358974359, 78.0448717948718, 72.59615384615384, 77.08333333333333, 71.15384615384616, 77.72435897435898, 71.63461538461539, 76.92307692307692, 74.51923076923077, 77.56410256410257, 78.68589743589743, 78.2051282051282, 77.56410256410257, 79.8076923076923, 79.96794871794872, 75.48076923076923, 81.41025641025641, 76.76282051282051, 78.68589743589743, 79.16666666666667, 79.6474358974359, 71.9551282051282, 76.4423076923077, 80.76923076923077, 81.41025641025641, 74.51923076923077, 79.48717948717949, 77.56410256410257, 71.7948717948718, 75.80128205128206, 81.89102564102564, 80.92948717948718, 80.44871794871794, 80.76923076923077, 82.8525641025641, 75.48076923076923, 77.24358974358974, 79.00641025641026, 84.2948717948718, 76.28205128205128, 82.05128205128206, 81.89102564102564, 82.37179487179488, 80.60897435897436, 77.24358974358974, 79.16666666666667, 80.12820512820512, 73.3974358974359, 80.92948717948718, 78.2051282051282, 76.6025641025641, 75.32051282051282, 77.40384615384616, 81.57051282051282, 81.57051282051282, 81.57051282051282, 80.44871794871794, 83.17307692307692, 82.8525641025641, 79.48717948717949, 79.00641025641026, 76.6025641025641, 78.0448717948718, 78.2051282051282, 76.6025641025641, 79.16666666666667, 76.6025641025641, 75.80128205128206, 75.64102564102564, 79.32692307692308, 79.8076923076923, 84.93589743589743, 80.28846153846153, 79.96794871794872, 75.96153846153847, 82.53205128205128, 79.48717948717949, 72.11538461538461, 80.76923076923077, 76.4423076923077, 80.76923076923077, 71.7948717948718, 78.84615384615384, 74.19871794871794, 76.12179487179488, 80.60897435897436, 71.9551282051282, 75.48076923076923, 75.16025641025641, 67.94871794871794, 75.64102564102564, 80.12820512820512, 74.19871794871794, 78.68589743589743, 77.24358974358974, 75.96153846153847, 72.91666666666667], 'test_losses': [0.6053773784198058, 0.620082643480064, 0.6852729123634979, 0.5200449933703893, 0.5289273134265573, 0.581830458357357, 0.50842016390883, 0.8466431582167459, 0.5616488520605251, 0.71373098788974, 0.6110180949863906, 0.7677556880546782, 0.6786866799259607, 0.6482411720401918, 0.6153972903027748, 0.7027013209702161, 0.5299169131769583, 0.664861802114711, 0.861199799336296, 0.5715040589342467, 0.7050364642480138, 0.6385938264198231, 0.6216874115306359, 0.6736429375954546, 0.8739745716694205, 0.7230327153908244, 0.49268628371497375, 0.59280741955482, 0.9290099328530069, 0.6896399845154986, 0.7582297671238521, 1.1066959065835527, 0.7714784459329587, 0.5594735256946968, 0.6149978816254775, 0.5534177039143964, 0.501821044995151, 0.5137498639904572, 0.8540845300064971, 0.8107741779240314, 0.6651208959329569, 0.5281518218297816, 0.8582606811969293, 0.5213731188127079, 0.5914368647149203, 0.7059300153515113, 0.6801103503129502, 0.8345466731812056, 0.8323909192027387, 0.7078727602358692, 1.0173703508979741, 0.6294456155036385, 0.7690864903040795, 0.951644072838229, 1.0175771272738647, 0.7723754947275245, 0.6472382358251474, 0.5759592681767968, 0.5453621262075523, 0.7614849355340061, 0.5569675888713951, 0.5663623821441061, 0.7952324650139371, 0.7614428195878505, 0.8327622853845614, 0.8444365860322693, 0.7719818022527855, 0.9410320604503608, 0.7244638531212845, 0.8613634580923155, 1.0525070399686602, 1.0292384520680804, 0.7679418751541585, 0.6816862112400719, 0.5049958170319979, 0.769802335276183, 0.6988234609126289, 1.078968079452655, 0.6584980641943193, 0.7439924589173946, 1.3608410846507188, 0.6923424784453789, 1.0687901325583926, 0.6123722778970543, 1.120354898213685, 0.9199023303883269, 1.2201463130017005, 1.001517448683817, 0.6648752702747469, 1.231438445887668, 1.0876920093867226, 1.1031666320773768, 1.6363119224423803, 1.0977020568001628, 0.7836758657228342, 1.0103350504587303, 0.7861966448753941, 0.9955115379360415, 1.0847792600704382, 1.2604117398538783], 'train_acss': [80.40644171779141, 85.00766871165644, 85.37193251533742, 85.2569018404908, 86.34969325153374, 86.6180981595092, 87.26993865030674, 86.73312883435582, 86.92484662576688, 87.4808282208589, 87.28911042944786, 87.9601226993865, 88.07515337423312, 88.51610429447852, 88.51610429447852, 88.8420245398773, 89.41717791411043, 90.06901840490798, 90.1457055214724, 90.27990797546012, 90.1457055214724, 91.12346625766871, 91.20015337423312, 91.545245398773, 91.73696319018404, 92.04371165644172, 92.00536809815951, 92.02453987730061, 92.15874233128834, 92.12039877300613, 92.29294478527608, 92.23542944785277, 92.54217791411043, 92.50383435582822, 93.04064417177914, 92.8872699386503, 92.8872699386503, 93.3282208588957, 93.00230061349693, 92.98312883435582, 93.75, 93.2898773006135, 93.36656441717791, 93.38573619631902, 93.51993865030674, 94.30598159509202, 93.46242331288343, 93.78834355828221, 93.55828220858896, 93.96088957055214, 93.59662576687117, 93.71165644171779, 93.84585889570552, 94.13343558282209, 94.26763803680981, 94.6127300613497, 94.42101226993866, 94.1909509202454, 94.34432515337423, 94.15260736196319, 94.670245398773, 94.2101226993865, 94.6319018404908, 94.72776073619632, 94.49769938650307, 94.57438650306749, 94.51687116564418, 94.53604294478528, 94.68941717791411, 94.49769938650307, 95.39877300613497, 94.90030674846626, 94.68941717791411, 94.88113496932516, 95.0920245398773, 94.6319018404908, 95.16871165644172, 94.90030674846626, 95.36042944785277, 95.32208588957056, 95.0536809815951, 95.28374233128834, 95.43711656441718, 94.95782208588957, 95.32208588957056, 95.47546012269939, 95.59049079754601, 95.5138036809816, 95.41794478527608, 95.39877300613497, 94.97699386503068, 95.64800613496932, 95.59049079754601, 95.64800613496932, 95.41794478527608, 95.34125766871166, 95.5713190184049, 95.82055214723927, 95.74386503067484, 95.62883435582822], 'train_losses': [0.4145647203949697, 0.3392179401282884, 0.32722860609575466, 0.3221699485720987, 0.31339535115720946, 0.30680893289719485, 0.2955032357306111, 0.2948629816714103, 0.2878749066538126, 0.2832981804814304, 0.27991685374822767, 0.2714143465844446, 0.2678191675147487, 0.2632005297196058, 0.2539698591979329, 0.2493394980767022, 0.24043015619822738, 0.22646996971545455, 0.2241571021596244, 0.2163644258766378, 0.21084464463638142, 0.20704586367069977, 0.20499975989166322, 0.1994381419224436, 0.19415383757930535, 0.1963173187894181, 0.19148416623692427, 0.1862233014149874, 0.18665432890997113, 0.1849774410219669, 0.18285467005920442, 0.1824352090727992, 0.17808520565387234, 0.1794873926496816, 0.17615477734908142, 0.17166926656982526, 0.16907873308720664, 0.16889002211956147, 0.16814950556262892, 0.1626116625462556, 0.16085605590534255, 0.16030717139582984, 0.16572432400077722, 0.1618936610700237, 0.15645574785067637, 0.14940689851339203, 0.15503754668490194, 0.1499016692635512, 0.15724708832496137, 0.14688884204166422, 0.1496968979480715, 0.1533669582378845, 0.150832077657034, 0.14768516458463826, 0.14607224078560418, 0.1424384219725502, 0.14461378867377633, 0.14772825730261765, 0.14344471861001967, 0.14037987970239255, 0.13748524405713552, 0.14204730764340304, 0.1325270629206178, 0.1389214078304922, 0.13929145879799962, 0.13089028064110117, 0.12897377369705404, 0.13114420329099177, 0.1345892871194912, 0.13214204035850421, 0.12137157749045395, 0.13191505101262277, 0.12897392699362473, 0.12322232222290419, 0.12454244599448541, 0.1262152338564036, 0.12534964191657705, 0.12686061685784517, 0.12252508060905963, 0.12039744584945972, 0.12236975056365307, 0.11938894091238642, 0.11744509623462296, 0.1218298618162413, 0.1154811930547374, 0.11332396020566612, 0.11099835433935613, 0.11489443178543074, 0.11711295438760144, 0.11458652454313745, 0.11716948517978883, 0.11247566001876946, 0.10809427161604249, 0.11194425507567707, 0.10991108448815459, 0.11708616247810237, 0.10657801510686872, 0.10909027151688984, 0.10602182358323184, 0.10583481188734334]}
2022-01-01 02:01:24.553801





===================================================================================

model = CCT(
        img_size=IMG_SIZE,
        embedding_dim=256,
        n_input_channels=1,
        n_conv_layers=1,
        kernel_size=3,
        stride=2,
        padding=1,
        pooling_kernel_size=3,
        pooling_stride=2,
        pooling_padding=1,
        num_layers=6,
        num_heads=4,
        mlp_radio=2.,
        num_classes=2,
        positional_embedding='none', # ['sine', 'learnable', 'none']
        )
print(summary(model.to(device), (1,IMG_SIZE ,IMG_SIZE)))


train_loader = DataLoader(CustomImageDataset('train.csv', '.', special_transform=transforms.Compose([tv.transforms.Grayscale(num_output_channels=1)]), transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.RandomRotation(.05)])), batch_size=64, shuffle=True, num_workers=2, pin_memory=True)

test_loader = DataLoader(CustomImageDataset('test.csv', '.', special_transform=transforms.Compose([tv.transforms.Grayscale(num_output_channels=1)])), batch_size=64, shuffle=False, num_workers=2, pin_memory=True)



cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 256, 32, 32]           2,304
              ReLU-2          [-1, 256, 32, 32]               0
         MaxPool2d-3          [-1, 256, 16, 16]               0
           Flatten-4             [-1, 256, 256]               0
         Tokenizer-5             [-1, 256, 256]               0
           Dropout-6             [-1, 256, 256]               0
         LayerNorm-7             [-1, 256, 256]             512
            Linear-8             [-1, 256, 768]         196,608
           Dropout-9          [-1, 4, 256, 256]               0
           Linear-10             [-1, 256, 256]          65,792
          Dropout-11             [-1, 256, 256]               0
        Attention-12             [-1, 256, 256]               0
         Identity-13             [-1, 256, 256]               0
        LayerNorm-14             [-1, 256, 256]             512
           Linear-15            [-1, 256, 1024]         263,168
          Dropout-16            [-1, 256, 1024]               0
           Linear-17             [-1, 256, 256]         262,400
          Dropout-18             [-1, 256, 256]               0
         Identity-19             [-1, 256, 256]               0
TransformerEncoderLayer-20             [-1, 256, 256]               0
        LayerNorm-21             [-1, 256, 256]             512
           Linear-22             [-1, 256, 768]         196,608
          Dropout-23          [-1, 4, 256, 256]               0
           Linear-24             [-1, 256, 256]          65,792
          Dropout-25             [-1, 256, 256]               0
        Attention-26             [-1, 256, 256]               0
         DropPath-27             [-1, 256, 256]               0
        LayerNorm-28             [-1, 256, 256]             512
           Linear-29            [-1, 256, 1024]         263,168
          Dropout-30            [-1, 256, 1024]               0
           Linear-31             [-1, 256, 256]         262,400
          Dropout-32             [-1, 256, 256]               0
         DropPath-33             [-1, 256, 256]               0
TransformerEncoderLayer-34             [-1, 256, 256]               0
        LayerNorm-35             [-1, 256, 256]             512
           Linear-36             [-1, 256, 768]         196,608
          Dropout-37          [-1, 4, 256, 256]               0
           Linear-38             [-1, 256, 256]          65,792
          Dropout-39             [-1, 256, 256]               0
        Attention-40             [-1, 256, 256]               0
         DropPath-41             [-1, 256, 256]               0
        LayerNorm-42             [-1, 256, 256]             512
           Linear-43            [-1, 256, 1024]         263,168
          Dropout-44            [-1, 256, 1024]               0
           Linear-45             [-1, 256, 256]         262,400
          Dropout-46             [-1, 256, 256]               0
         DropPath-47             [-1, 256, 256]               0
TransformerEncoderLayer-48             [-1, 256, 256]               0
        LayerNorm-49             [-1, 256, 256]             512
           Linear-50             [-1, 256, 768]         196,608
          Dropout-51          [-1, 4, 256, 256]               0
           Linear-52             [-1, 256, 256]          65,792
          Dropout-53             [-1, 256, 256]               0
        Attention-54             [-1, 256, 256]               0
         DropPath-55             [-1, 256, 256]               0
        LayerNorm-56             [-1, 256, 256]             512
           Linear-57            [-1, 256, 1024]         263,168
          Dropout-58            [-1, 256, 1024]               0
           Linear-59             [-1, 256, 256]         262,400
          Dropout-60             [-1, 256, 256]               0
         DropPath-61             [-1, 256, 256]               0
TransformerEncoderLayer-62             [-1, 256, 256]               0
        LayerNorm-63             [-1, 256, 256]             512
           Linear-64             [-1, 256, 768]         196,608
          Dropout-65          [-1, 4, 256, 256]               0
           Linear-66             [-1, 256, 256]          65,792
          Dropout-67             [-1, 256, 256]               0
        Attention-68             [-1, 256, 256]               0
         DropPath-69             [-1, 256, 256]               0
        LayerNorm-70             [-1, 256, 256]             512
           Linear-71            [-1, 256, 1024]         263,168
          Dropout-72            [-1, 256, 1024]               0
           Linear-73             [-1, 256, 256]         262,400
          Dropout-74             [-1, 256, 256]               0
         DropPath-75             [-1, 256, 256]               0
TransformerEncoderLayer-76             [-1, 256, 256]               0
        LayerNorm-77             [-1, 256, 256]             512
           Linear-78             [-1, 256, 768]         196,608
          Dropout-79          [-1, 4, 256, 256]               0
           Linear-80             [-1, 256, 256]          65,792
          Dropout-81             [-1, 256, 256]               0
        Attention-82             [-1, 256, 256]               0
         DropPath-83             [-1, 256, 256]               0
        LayerNorm-84             [-1, 256, 256]             512
           Linear-85            [-1, 256, 1024]         263,168
          Dropout-86            [-1, 256, 1024]               0
           Linear-87             [-1, 256, 256]         262,400
          Dropout-88             [-1, 256, 256]               0
         DropPath-89             [-1, 256, 256]               0
TransformerEncoderLayer-90             [-1, 256, 256]               0
        LayerNorm-91             [-1, 256, 256]             512
           Linear-92               [-1, 256, 1]             257
           Linear-93                    [-1, 2]             514
TransformerClassifier-94                    [-1, 2]               0
================================================================
Total params: 4,737,539
Trainable params: 4,737,539
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.02
Forward/backward pass size (MB): 81.50
Params size (MB): 18.07
Estimated Total Size (MB): 99.59
----------------------------------------------------------------
None
2022-01-01 14:19:58.555815
Epoch: [0]	 Loss 0.4922 (0.5449)	 acc 68.750 (73.198)
Test	  accuracy: 67.147 (Err: 0.774 )

Epoch: [1]	 Loss 0.4683 (0.4741)	 acc 84.375 (76.783)
Test	  accuracy: 64.263 (Err: 0.699 )

Epoch: [2]	 Loss 0.4175 (0.4485)	 acc 81.250 (77.895)
Test	  accuracy: 67.949 (Err: 0.668 )

Epoch: [3]	 Loss 0.2502 (0.4301)	 acc 90.625 (79.908)
Test	  accuracy: 73.397 (Err: 0.575 )

Epoch: [4]	 Loss 0.4367 (0.3951)	 acc 84.375 (81.423)
Test	  accuracy: 75.481 (Err: 0.472 )

Epoch: [5]	 Loss 0.2687 (0.3548)	 acc 84.375 (83.397)
Test	  accuracy: 79.167 (Err: 0.433 )

Epoch: [6]	 Loss 0.4881 (0.3166)	 acc 68.750 (85.544)
Test	  accuracy: 82.692 (Err: 0.370 )

Epoch: [7]	 Loss 0.2028 (0.3113)	 acc 93.750 (85.985)
Test	  accuracy: 82.372 (Err: 0.385 )

Epoch: [8]	 Loss 0.4742 (0.2828)	 acc 81.250 (87.232)
Test	  accuracy: 83.814 (Err: 0.375 )

Epoch: [9]	 Loss 0.3804 (0.2927)	 acc 78.125 (86.963)
Test	  accuracy: 80.769 (Err: 0.424 )

Epoch: [10]	 Loss 0.4019 (0.2640)	 acc 81.250 (88.612)
Test	  accuracy: 83.974 (Err: 0.395 )

Epoch: [11]	 Loss 0.1318 (0.2594)	 acc 96.875 (88.995)
Test	  accuracy: 80.769 (Err: 0.456 )

Epoch: [12]	 Loss 0.5028 (0.2510)	 acc 81.250 (89.417)
Test	  accuracy: 79.647 (Err: 0.507 )

Epoch: [13]	 Loss 0.4532 (0.2494)	 acc 71.875 (89.225)
Test	  accuracy: 83.974 (Err: 0.395 )

Epoch: [14]	 Loss 0.4086 (0.2429)	 acc 87.500 (89.705)
Test	  accuracy: 82.853 (Err: 0.385 )

Epoch: [15]	 Loss 0.4444 (0.2414)	 acc 93.750 (89.781)
Test	  accuracy: 80.609 (Err: 0.483 )

Epoch: [16]	 Loss 0.2240 (0.2357)	 acc 84.375 (90.012)
Test	  accuracy: 78.686 (Err: 0.631 )

Epoch: [17]	 Loss 0.1922 (0.2368)	 acc 90.625 (90.165)
Test	  accuracy: 80.769 (Err: 0.528 )

Epoch: [18]	 Loss 0.2290 (0.2243)	 acc 93.750 (90.644)
Test	  accuracy: 83.494 (Err: 0.432 )

Epoch: [19]	 Loss 0.2050 (0.2468)	 acc 90.625 (89.264)
Test	  accuracy: 83.333 (Err: 0.393 )

Epoch: [20]	 Loss 0.3330 (0.2259)	 acc 90.625 (91.066)
Test	  accuracy: 82.853 (Err: 0.478 )

Epoch: [21]	 Loss 0.3158 (0.2218)	 acc 87.500 (90.510)
Test	  accuracy: 82.853 (Err: 0.449 )

Epoch: [22]	 Loss 0.3656 (0.2331)	 acc 84.375 (90.510)
Test	  accuracy: 76.122 (Err: 0.659 )

Epoch: [23]	 Loss 0.4288 (0.2214)	 acc 81.250 (90.452)
Test	  accuracy: 83.654 (Err: 0.405 )

Epoch: [24]	 Loss 0.3433 (0.2021)	 acc 87.500 (91.794)
Test	  accuracy: 82.692 (Err: 0.597 )

Epoch: [25]	 Loss 0.1399 (0.2205)	 acc 96.875 (90.893)
Test	  accuracy: 84.455 (Err: 0.482 )

Epoch: [26]	 Loss 0.1187 (0.2026)	 acc 93.750 (91.660)
Test	  accuracy: 83.013 (Err: 0.538 )

Epoch: [27]	 Loss 0.3171 (0.2193)	 acc 87.500 (91.085)
Test	  accuracy: 84.295 (Err: 0.361 )

Epoch: [28]	 Loss 0.3044 (0.2310)	 acc 84.375 (90.012)
Test	  accuracy: 81.090 (Err: 0.550 )

Epoch: [29]	 Loss 0.1574 (0.2076)	 acc 93.750 (91.315)
Test	  accuracy: 81.250 (Err: 0.612 )

Epoch: [30]	 Loss 0.1622 (0.2057)	 acc 93.750 (91.584)
Test	  accuracy: 78.846 (Err: 0.716 )

Epoch: [31]	 Loss 0.2807 (0.1938)	 acc 84.375 (92.581)
Test	  accuracy: 73.718 (Err: 0.751 )

Epoch: [32]	 Loss 0.2665 (0.2077)	 acc 87.500 (91.545)
Test	  accuracy: 78.846 (Err: 0.632 )

Epoch: [33]	 Loss 0.2741 (0.2047)	 acc 90.625 (91.929)
Test	  accuracy: 82.532 (Err: 0.478 )

Epoch: [34]	 Loss 0.2190 (0.1983)	 acc 90.625 (92.005)
Test	  accuracy: 77.083 (Err: 0.727 )

Epoch: [35]	 Loss 0.1654 (0.1989)	 acc 90.625 (91.794)
Test	  accuracy: 79.327 (Err: 0.889 )

Epoch: [36]	 Loss 0.1895 (0.1964)	 acc 96.875 (92.523)
Test	  accuracy: 82.853 (Err: 0.589 )

Epoch: [37]	 Loss 0.1982 (0.1947)	 acc 93.750 (92.044)
Test	  accuracy: 83.013 (Err: 0.629 )

Epoch: [38]	 Loss 0.1875 (0.1972)	 acc 93.750 (92.120)
Test	  accuracy: 77.404 (Err: 0.611 )

Epoch: [39]	 Loss 0.2710 (0.1998)	 acc 84.375 (91.852)
Test	  accuracy: 82.853 (Err: 0.572 )

Epoch: [40]	 Loss 0.0832 (0.1877)	 acc 100.000 (92.523)
Test	  accuracy: 81.090 (Err: 0.706 )

Epoch: [41]	 Loss 0.1566 (0.1947)	 acc 93.750 (92.101)
Test	  accuracy: 79.327 (Err: 0.866 )

Epoch: [42]	 Loss 0.1885 (0.1903)	 acc 96.875 (92.331)
Test	  accuracy: 80.769 (Err: 0.446 )

Epoch: [43]	 Loss 0.1893 (0.1938)	 acc 90.625 (92.120)
Test	  accuracy: 80.769 (Err: 0.624 )

Epoch: [44]	 Loss 0.2174 (0.1763)	 acc 93.750 (93.156)
Test	  accuracy: 80.288 (Err: 0.801 )

Epoch: [45]	 Loss 0.1081 (0.1848)	 acc 96.875 (92.657)
Test	  accuracy: 74.359 (Err: 1.082 )

Epoch: [46]	 Loss 0.2764 (0.1967)	 acc 90.625 (92.120)
Test	  accuracy: 83.814 (Err: 0.608 )

Epoch: [47]	 Loss 0.2260 (0.1731)	 acc 87.500 (93.347)
Test	  accuracy: 80.769 (Err: 0.735 )

Epoch: [48]	 Loss 0.0746 (0.1771)	 acc 96.875 (92.906)
Test	  accuracy: 77.244 (Err: 1.032 )

Epoch: [49]	 Loss 0.0654 (0.1701)	 acc 96.875 (93.386)
Test	  accuracy: 79.647 (Err: 0.780 )

Epoch: [50]	 Loss 0.2011 (0.1850)	 acc 90.625 (92.906)
Test	  accuracy: 86.699 (Err: 0.414 )

Epoch: [51]	 Loss 0.4450 (0.1790)	 acc 87.500 (93.098)
Test	  accuracy: 77.724 (Err: 1.066 )

Epoch: [52]	 Loss 0.0888 (0.1840)	 acc 96.875 (92.523)
Test	  accuracy: 79.968 (Err: 0.759 )

Epoch: [53]	 Loss 0.0510 (0.1721)	 acc 100.000 (93.443)
Test	  accuracy: 79.327 (Err: 0.544 )

Epoch: [54]	 Loss 0.1967 (0.1693)	 acc 90.625 (93.367)
Test	  accuracy: 78.846 (Err: 0.939 )

Epoch: [55]	 Loss 0.0833 (0.1797)	 acc 93.750 (92.446)
Test	  accuracy: 80.769 (Err: 0.716 )

Epoch: [56]	 Loss 0.1827 (0.1708)	 acc 87.500 (93.252)
Test	  accuracy: 78.045 (Err: 0.730 )

Epoch: [57]	 Loss 0.1283 (0.1721)	 acc 93.750 (93.098)
Test	  accuracy: 75.321 (Err: 0.912 )

Epoch: [58]	 Loss 0.1277 (0.1774)	 acc 93.750 (92.983)
Test	  accuracy: 78.686 (Err: 0.652 )

Epoch: [59]	 Loss 0.3437 (0.1744)	 acc 87.500 (93.194)
Test	  accuracy: 81.891 (Err: 0.620 )

Epoch: [60]	 Loss 0.1782 (0.1781)	 acc 90.625 (92.868)
Test	  accuracy: 77.404 (Err: 0.661 )

Epoch: [61]	 Loss 0.2911 (0.1783)	 acc 87.500 (93.079)
Test	  accuracy: 72.436 (Err: 1.138 )

Epoch: [62]	 Loss 0.0551 (0.1822)	 acc 100.000 (92.926)
Test	  accuracy: 78.526 (Err: 0.699 )

Epoch: [63]	 Loss 0.1436 (0.1631)	 acc 90.625 (93.501)
Test	  accuracy: 76.442 (Err: 0.787 )

Epoch: [64]	 Loss 0.1044 (0.1667)	 acc 96.875 (93.597)
Test	  accuracy: 76.923 (Err: 0.848 )

Epoch: [65]	 Loss 0.1445 (0.1609)	 acc 93.750 (93.539)
Test	  accuracy: 80.769 (Err: 0.528 )

Epoch: [66]	 Loss 0.0742 (0.1632)	 acc 96.875 (93.367)
Test	  accuracy: 74.038 (Err: 0.970 )

Epoch: [67]	 Loss 0.1500 (0.1643)	 acc 93.750 (93.577)
Test	  accuracy: 73.878 (Err: 0.972 )

Epoch: [68]	 Loss 0.0295 (0.1538)	 acc 100.000 (94.248)
Test	  accuracy: 79.968 (Err: 0.684 )

Epoch: [69]	 Loss 0.1446 (0.1607)	 acc 93.750 (93.482)
Test	  accuracy: 73.237 (Err: 1.039 )

Epoch: [70]	 Loss 0.0497 (0.1721)	 acc 96.875 (93.021)
Test	  accuracy: 70.353 (Err: 1.325 )

Epoch: [71]	 Loss 0.1188 (0.1604)	 acc 96.875 (93.616)
Test	  accuracy: 75.160 (Err: 0.923 )

Epoch: [72]	 Loss 0.1575 (0.1561)	 acc 93.750 (93.980)
Test	  accuracy: 76.282 (Err: 0.625 )

Epoch: [73]	 Loss 0.1705 (0.1471)	 acc 93.750 (94.479)
Test	  accuracy: 74.199 (Err: 1.051 )

Epoch: [74]	 Loss 0.1485 (0.1527)	 acc 93.750 (94.038)
Test	  accuracy: 71.955 (Err: 1.169 )

Epoch: [75]	 Loss 0.2362 (0.1556)	 acc 93.750 (93.731)
Test	  accuracy: 76.603 (Err: 0.817 )

Epoch: [76]	 Loss 0.1667 (0.1545)	 acc 90.625 (93.961)
Test	  accuracy: 77.404 (Err: 0.880 )

Epoch: [77]	 Loss 0.0422 (0.1491)	 acc 100.000 (94.133)
Test	  accuracy: 77.244 (Err: 0.890 )

Epoch: [78]	 Loss 0.0647 (0.1522)	 acc 100.000 (93.788)
Test	  accuracy: 77.885 (Err: 0.747 )

Epoch: [79]	 Loss 0.0947 (0.1549)	 acc 96.875 (93.788)
Test	  accuracy: 79.968 (Err: 0.570 )

Epoch: [80]	 Loss 0.2940 (0.1570)	 acc 87.500 (94.325)
Test	  accuracy: 83.013 (Err: 0.603 )

Epoch: [81]	 Loss 0.3565 (0.1442)	 acc 90.625 (94.517)
Test	  accuracy: 77.404 (Err: 0.893 )

Epoch: [82]	 Loss 0.1512 (0.1382)	 acc 87.500 (94.325)
Test	  accuracy: 81.090 (Err: 0.688 )

Epoch: [83]	 Loss 0.2902 (0.1451)	 acc 90.625 (94.325)
Test	  accuracy: 71.955 (Err: 1.182 )

Epoch: [84]	 Loss 0.1191 (0.1444)	 acc 96.875 (94.268)
Test	  accuracy: 74.359 (Err: 0.939 )

Epoch: [85]	 Loss 0.0854 (0.1462)	 acc 100.000 (94.018)
Test	  accuracy: 80.288 (Err: 0.565 )

Epoch: [86]	 Loss 0.2742 (0.1477)	 acc 93.750 (94.479)
Test	  accuracy: 77.885 (Err: 0.660 )

Epoch: [87]	 Loss 0.0751 (0.1440)	 acc 93.750 (94.421)
Test	  accuracy: 80.128 (Err: 0.578 )

Epoch: [88]	 Loss 0.1390 (0.1541)	 acc 93.750 (93.769)
Test	  accuracy: 77.564 (Err: 0.897 )

Epoch: [89]	 Loss 0.2218 (0.1424)	 acc 93.750 (94.632)
Test	  accuracy: 76.442 (Err: 0.865 )

Epoch: [90]	 Loss 0.1026 (0.1458)	 acc 96.875 (94.114)
Test	  accuracy: 75.481 (Err: 0.979 )

Epoch: [91]	 Loss 0.2405 (0.1432)	 acc 84.375 (94.402)
Test	  accuracy: 78.205 (Err: 0.675 )

Epoch: [92]	 Loss 0.1624 (0.1494)	 acc 93.750 (94.172)
Test	  accuracy: 78.045 (Err: 0.737 )

Epoch: [93]	 Loss 0.0586 (0.1362)	 acc 96.875 (94.363)
Test	  accuracy: 75.160 (Err: 0.882 )

Epoch: [94]	 Loss 0.1072 (0.1314)	 acc 96.875 (94.996)
Test	  accuracy: 78.526 (Err: 0.802 )

Epoch: [95]	 Loss 0.0571 (0.1392)	 acc 96.875 (94.574)
Test	  accuracy: 77.885 (Err: 0.702 )

Epoch: [96]	 Loss 0.0518 (0.1342)	 acc 100.000 (94.574)
Test	  accuracy: 79.487 (Err: 0.635 )

Epoch: [97]	 Loss 0.1037 (0.1393)	 acc 96.875 (94.421)
Test	  accuracy: 71.154 (Err: 1.225 )

Epoch: [98]	 Loss 0.0750 (0.1389)	 acc 96.875 (94.344)
Test	  accuracy: 77.083 (Err: 0.874 )

Epoch: [99]	 Loss 0.0178 (0.1370)	 acc 100.000 (94.594)
Test	  accuracy: 76.763 (Err: 0.919 )

{'best_acc': 86.69871775309245, 'test_accs': [67.14743587298271, 64.26282048836732, 67.94871789981157, 73.39743575071677, 75.48076883951823, 79.16666656885391, 82.6923076923077, 82.37179487179488, 83.81410217285156, 80.76923067141801, 83.97435897435898, 80.76923057360527, 79.64743579962315, 83.97435858310797, 82.8525637113131, 80.6089742611616, 78.68589724027194, 80.76923067141801, 83.49358974358974, 83.33333333333333, 82.8525639069386, 82.8525641025641, 76.12179487179488, 83.65384576259515, 82.69230749668219, 84.4551278138772, 83.01282031719501, 84.2948717948718, 81.08974329630534, 81.249999608749, 78.84615355271559, 73.71794852232321, 78.8461537483411, 82.53205089080028, 77.08333323552058, 79.32692288129758, 82.8525639069386, 83.01282051282051, 77.40384615384616, 82.8525639069386, 81.08974319849258, 79.32692278348483, 80.76923067141801, 80.76923067141801, 80.28846144064879, 74.35897435897436, 83.81410236847707, 80.76923057360527, 77.24358954796425, 79.64743579962315, 86.69871775309245, 77.72435897435898, 79.96794862013597, 79.32692297911032, 78.84615365052835, 80.76923047579251, 78.04487150143355, 75.32051282051282, 78.68589724027194, 81.89102524977464, 77.40384595822066, 72.43589728917831, 78.52564083001552, 76.44230759449495, 76.92307672745142, 80.76923037797977, 74.03846153846153, 73.87820493257962, 79.96794842451047, 73.23717938936673, 70.35256400475136, 75.1602562146309, 76.28205108642578, 74.1987178509052, 71.95512805840907, 76.60256400475136, 77.4038460560334, 77.24358964577699, 77.88461528680263, 79.96794871794872, 83.01282021938226, 77.40384615384616, 81.08974329630534, 71.95512810731546, 74.35897421225523, 80.28846134283604, 77.88461518898988, 80.12820503039238, 77.56410246628981, 76.44230749668219, 75.48076923076923, 78.2051280095027, 78.04487169705905, 75.16025641025641, 78.52564102564102, 77.88461518898988, 79.48717919374124, 71.15384615384616, 77.08333313770784, 76.76282041500777], 'test_losses': [0.7743291029563317, 0.699043027865581, 0.6676001243102245, 0.5748855058963482, 0.47166457237341464, 0.4334143002827962, 0.36991655902984816, 0.38518732480513745, 0.375285879159585, 0.42442567761127764, 0.3953956166903178, 0.4556124462531163, 0.5066208908191094, 0.3946281969547272, 0.3854326804478963, 0.48288133587592685, 0.6313606194960766, 0.5275908173658909, 0.4322327719284938, 0.393108714849521, 0.4781418794240707, 0.44927974083484745, 0.6592329786374018, 0.40481258355654204, 0.596546220473754, 0.4820105028458131, 0.5376292956181061, 0.3611557017534207, 0.5502205009643848, 0.6115515201519697, 0.71617653239996, 0.7506494743701739, 0.6315754136213889, 0.4779307395219803, 0.7269084854767873, 0.8889787181829795, 0.5891661032652243, 0.6294871897269518, 0.6112507621829326, 0.5718083198253925, 0.7055914551019669, 0.8664770519886261, 0.4456792630446263, 0.623776742281058, 0.8012479176888099, 1.0820441668232281, 0.6083684892226489, 0.7350792772112749, 1.0320571960929112, 0.7800217114197903, 0.41385440566600895, 1.0662427546981819, 0.7589274618106011, 0.5443807603457035, 0.9388166673672504, 0.716170052687327, 0.7303621226396316, 0.9118980950652025, 0.6518091230820386, 0.6197813367232298, 0.6610869333530084, 1.137865820278724, 0.69898110972001, 0.7871576972687856, 0.848495172479978, 0.5283876699514878, 0.9704578767220179, 0.9720775951655247, 0.6841304130279101, 1.0385928193155007, 1.324514458911159, 0.9233900339653095, 0.6248021309192364, 1.0514974577638965, 1.1688908068105006, 0.8167245089052579, 0.8802393541599696, 0.8895640574777738, 0.7467186649640402, 0.569732727530675, 0.6026280864309042, 0.8925553388320483, 0.687950730896913, 1.1820655547512264, 0.9391572115321954, 0.5647960460911958, 0.6604362174104421, 0.5781769194664099, 0.8966483281782041, 0.8651348805198302, 0.9794931643857405, 0.6754533755473602, 0.7370492225656142, 0.8823911537631199, 0.8018008751364855, 0.7016063536015841, 0.6345257107646037, 1.2253420350070183, 0.8740310155523893, 0.9192176671364368], 'train_acss': [73.1978527607362, 76.7829754601227, 77.89493865030674, 79.9079754601227, 81.42254601226993, 83.39723926380368, 85.54447852760737, 85.98542944785277, 87.23159509202453, 86.96319018404908, 88.61196319018404, 88.99539877300613, 89.41717791411043, 89.22546012269939, 89.704754601227, 89.78144171779141, 90.01150306748467, 90.1648773006135, 90.6441717791411, 89.2638036809816, 91.0659509202454, 90.50996932515338, 90.50996932515338, 90.45245398773007, 91.79447852760737, 90.89340490797547, 91.66027607361963, 91.0851226993865, 90.01150306748467, 91.31518404907976, 91.58358895705521, 92.58052147239263, 91.545245398773, 91.9286809815951, 92.00536809815951, 91.79447852760737, 92.52300613496932, 92.04371165644172, 92.12039877300613, 91.85199386503068, 92.52300613496932, 92.10122699386503, 92.33128834355828, 92.12039877300613, 93.15567484662577, 92.65720858895706, 92.12039877300613, 93.34739263803681, 92.90644171779141, 93.38573619631902, 92.90644171779141, 93.09815950920246, 92.52300613496932, 93.44325153374233, 93.36656441717791, 92.4463190184049, 93.25153374233129, 93.09815950920246, 92.98312883435582, 93.19401840490798, 92.8680981595092, 93.07898773006134, 92.92561349693251, 93.50076687116564, 93.59662576687117, 93.53911042944786, 93.36656441717791, 93.57745398773007, 94.24846625766871, 93.48159509202453, 93.02147239263803, 93.61579754601227, 93.98006134969326, 94.47852760736197, 94.03757668711657, 93.7308282208589, 93.96088957055214, 94.13343558282209, 93.78834355828221, 93.78834355828221, 94.32515337423312, 94.51687116564418, 94.32515337423312, 94.32515337423312, 94.26763803680981, 94.01840490797547, 94.47852760736197, 94.42101226993866, 93.7691717791411, 94.6319018404908, 94.11426380368098, 94.40184049079754, 94.1717791411043, 94.36349693251533, 94.99616564417178, 94.57438650306749, 94.57438650306749, 94.42101226993866, 94.34432515337423, 94.59355828220859], 'train_losses': [0.5448647521756178, 0.474106300462243, 0.4485420372588503, 0.43008224455856836, 0.39510855682057106, 0.35482093618691335, 0.31655990254659594, 0.3112597162921005, 0.2828054166644629, 0.2926870573517735, 0.2640406791052204, 0.2594455906584219, 0.2509579695075568, 0.2493672118596504, 0.242906306693159, 0.24137356292250697, 0.2357402111123676, 0.23682498054270365, 0.22433336179680619, 0.24684354461775235, 0.22591321929100833, 0.2218425824232628, 0.23311078877536798, 0.221442123497922, 0.20212609563137124, 0.22050437532319614, 0.20255227830329556, 0.21932337638790622, 0.23098991345042832, 0.2076330508556834, 0.20573889660688996, 0.1938167269244516, 0.20768265298173472, 0.20466235828545928, 0.19828059693421324, 0.19892125754999967, 0.19642056424193588, 0.19472024548638817, 0.19722647169616325, 0.19977039263292323, 0.1876536336687445, 0.19468671659384768, 0.19029843976344069, 0.1937692746794297, 0.17628909674890203, 0.18480869469467115, 0.19665272479042686, 0.17305813839464831, 0.17707201166569822, 0.1700913401926222, 0.18502724284957522, 0.17899162866220883, 0.18402020080140763, 0.17205342570422616, 0.16926973230816836, 0.1797274139029848, 0.17077161339711558, 0.1720575927956704, 0.17737923896203012, 0.17444055609725004, 0.17809831730427186, 0.1783337241118671, 0.1821664935606389, 0.16310391725938014, 0.16666135566731904, 0.1609313655758928, 0.16319415416637081, 0.1642996196640781, 0.1538106028470525, 0.16072318017299922, 0.17205288688555084, 0.1603915717887001, 0.15609256548384215, 0.1470905065627917, 0.1527364106814554, 0.1555972336991433, 0.15448993183900975, 0.1491027161981796, 0.15217063036615863, 0.15486298538058813, 0.15704209170092834, 0.14416096775444007, 0.13816523094850083, 0.14507207708673242, 0.1443843448454617, 0.14617476450062236, 0.14766441824977383, 0.14399304501483776, 0.15411432978748543, 0.1423887865949262, 0.1457872918229893, 0.1431764095930234, 0.1494069715584714, 0.13623223314347443, 0.13138156957878658, 0.13923852815309917, 0.13415889579094262, 0.1392583693920469, 0.1388706297276576, 0.13701214857308044]}
2022-01-01 14:58:12.437808



======================================================================
lr = .0001
model = CCT(
        img_size=IMG_SIZE,
        embedding_dim=256,
        n_input_channels=1,
        n_conv_layers=1,
        kernel_size=3,
        stride=2,
        padding=1,
        pooling_kernel_size=3,
        pooling_stride=2,
        pooling_padding=1,
        num_layers=6,
        num_heads=4,
        mlp_radio=2.,
        num_classes=2,
        positional_embedding='sine', # ['sine', 'learnable', 'none']
        )
print(summary(model.to(device), (1,IMG_SIZE ,IMG_SIZE)))


train_loader = DataLoader(CustomImageDataset('train.csv', '.', special_transform=transforms.Compose([tv.transforms.Grayscale(num_output_channels=1)]), transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.RandomRotation(.05)])), batch_size=64, shuffle=True, num_workers=2, pin_memory=True)

test_loader = DataLoader(CustomImageDataset('test.csv', '.', special_transform=transforms.Compose([tv.transforms.Grayscale(num_output_channels=1)])), batch_size=64, shuffle=False, num_workers=2, pin_memory=True)






cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 256, 32, 32]           2,304
              ReLU-2          [-1, 256, 32, 32]               0
         MaxPool2d-3          [-1, 256, 16, 16]               0
           Flatten-4             [-1, 256, 256]               0
         Tokenizer-5             [-1, 256, 256]               0
           Dropout-6             [-1, 256, 256]               0
         LayerNorm-7             [-1, 256, 256]             512
            Linear-8             [-1, 256, 768]         196,608
           Dropout-9          [-1, 4, 256, 256]               0
           Linear-10             [-1, 256, 256]          65,792
          Dropout-11             [-1, 256, 256]               0
        Attention-12             [-1, 256, 256]               0
         Identity-13             [-1, 256, 256]               0
        LayerNorm-14             [-1, 256, 256]             512
           Linear-15            [-1, 256, 1024]         263,168
          Dropout-16            [-1, 256, 1024]               0
           Linear-17             [-1, 256, 256]         262,400
          Dropout-18             [-1, 256, 256]               0
         Identity-19             [-1, 256, 256]               0
TransformerEncoderLayer-20             [-1, 256, 256]               0
        LayerNorm-21             [-1, 256, 256]             512
           Linear-22             [-1, 256, 768]         196,608
          Dropout-23          [-1, 4, 256, 256]               0
           Linear-24             [-1, 256, 256]          65,792
          Dropout-25             [-1, 256, 256]               0
        Attention-26             [-1, 256, 256]               0
         DropPath-27             [-1, 256, 256]               0
        LayerNorm-28             [-1, 256, 256]             512
           Linear-29            [-1, 256, 1024]         263,168
          Dropout-30            [-1, 256, 1024]               0
           Linear-31             [-1, 256, 256]         262,400
          Dropout-32             [-1, 256, 256]               0
         DropPath-33             [-1, 256, 256]               0
TransformerEncoderLayer-34             [-1, 256, 256]               0
        LayerNorm-35             [-1, 256, 256]             512
           Linear-36             [-1, 256, 768]         196,608
          Dropout-37          [-1, 4, 256, 256]               0
           Linear-38             [-1, 256, 256]          65,792
          Dropout-39             [-1, 256, 256]               0
        Attention-40             [-1, 256, 256]               0
         DropPath-41             [-1, 256, 256]               0
        LayerNorm-42             [-1, 256, 256]             512
           Linear-43            [-1, 256, 1024]         263,168
          Dropout-44            [-1, 256, 1024]               0
           Linear-45             [-1, 256, 256]         262,400
          Dropout-46             [-1, 256, 256]               0
         DropPath-47             [-1, 256, 256]               0
TransformerEncoderLayer-48             [-1, 256, 256]               0
        LayerNorm-49             [-1, 256, 256]             512
           Linear-50             [-1, 256, 768]         196,608
          Dropout-51          [-1, 4, 256, 256]               0
           Linear-52             [-1, 256, 256]          65,792
          Dropout-53             [-1, 256, 256]               0
        Attention-54             [-1, 256, 256]               0
         DropPath-55             [-1, 256, 256]               0
        LayerNorm-56             [-1, 256, 256]             512
           Linear-57            [-1, 256, 1024]         263,168
          Dropout-58            [-1, 256, 1024]               0
           Linear-59             [-1, 256, 256]         262,400
          Dropout-60             [-1, 256, 256]               0
         DropPath-61             [-1, 256, 256]               0
TransformerEncoderLayer-62             [-1, 256, 256]               0
        LayerNorm-63             [-1, 256, 256]             512
           Linear-64             [-1, 256, 768]         196,608
          Dropout-65          [-1, 4, 256, 256]               0
           Linear-66             [-1, 256, 256]          65,792
          Dropout-67             [-1, 256, 256]               0
        Attention-68             [-1, 256, 256]               0
         DropPath-69             [-1, 256, 256]               0
        LayerNorm-70             [-1, 256, 256]             512
           Linear-71            [-1, 256, 1024]         263,168
          Dropout-72            [-1, 256, 1024]               0
           Linear-73             [-1, 256, 256]         262,400
          Dropout-74             [-1, 256, 256]               0
         DropPath-75             [-1, 256, 256]               0
TransformerEncoderLayer-76             [-1, 256, 256]               0
        LayerNorm-77             [-1, 256, 256]             512
           Linear-78             [-1, 256, 768]         196,608
          Dropout-79          [-1, 4, 256, 256]               0
           Linear-80             [-1, 256, 256]          65,792
          Dropout-81             [-1, 256, 256]               0
        Attention-82             [-1, 256, 256]               0
         DropPath-83             [-1, 256, 256]               0
        LayerNorm-84             [-1, 256, 256]             512
           Linear-85            [-1, 256, 1024]         263,168
          Dropout-86            [-1, 256, 1024]               0
           Linear-87             [-1, 256, 256]         262,400
          Dropout-88             [-1, 256, 256]               0
         DropPath-89             [-1, 256, 256]               0
TransformerEncoderLayer-90             [-1, 256, 256]               0
        LayerNorm-91             [-1, 256, 256]             512
           Linear-92               [-1, 256, 1]             257
           Linear-93                    [-1, 2]             514
TransformerClassifier-94                    [-1, 2]               0
================================================================
Total params: 4,737,539
Trainable params: 4,737,539
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.02
Forward/backward pass size (MB): 81.50
Params size (MB): 18.07
Estimated Total Size (MB): 99.59
----------------------------------------------------------------
None
2022-01-01 16:06:12.431291
Epoch: [0]	 Loss 0.4811 (0.5556)	 acc 84.375 (72.757)
Test	  accuracy: 68.590 (Err: 0.723 )

Epoch: [1]	 Loss 0.5925 (0.4817)	 acc 62.500 (76.131)
Test	  accuracy: 71.955 (Err: 0.638 )

Epoch: [2]	 Loss 0.5688 (0.4563)	 acc 68.750 (77.857)
Test	  accuracy: 67.468 (Err: 0.655 )

Epoch: [3]	 Loss 0.5053 (0.4222)	 acc 75.000 (79.946)
Test	  accuracy: 67.308 (Err: 0.691 )

Epoch: [4]	 Loss 0.4451 (0.4030)	 acc 81.250 (81.250)
Test	  accuracy: 78.686 (Err: 0.440 )

Epoch: [5]	 Loss 0.2502 (0.3328)	 acc 96.875 (85.123)
Test	  accuracy: 79.487 (Err: 0.430 )

Epoch: [6]	 Loss 0.4975 (0.3156)	 acc 78.125 (85.794)
Test	  accuracy: 81.891 (Err: 0.431 )

Epoch: [7]	 Loss 0.2460 (0.2975)	 acc 84.375 (86.369)
Test	  accuracy: 78.846 (Err: 0.499 )

Epoch: [8]	 Loss 0.2492 (0.2721)	 acc 87.500 (87.826)
Test	  accuracy: 75.962 (Err: 0.804 )

Epoch: [9]	 Loss 0.3860 (0.2880)	 acc 78.125 (87.366)
Test	  accuracy: 83.494 (Err: 0.471 )

Epoch: [10]	 Loss 0.3580 (0.2785)	 acc 81.250 (87.673)
Test	  accuracy: 82.853 (Err: 0.441 )

Epoch: [11]	 Loss 0.4155 (0.2532)	 acc 84.375 (88.823)
Test	  accuracy: 83.814 (Err: 0.474 )

Epoch: [12]	 Loss 0.3750 (0.2695)	 acc 71.875 (88.018)
Test	  accuracy: 81.891 (Err: 0.536 )

Epoch: [13]	 Loss 0.0564 (0.2596)	 acc 100.000 (88.861)
Test	  accuracy: 82.853 (Err: 0.423 )

Epoch: [14]	 Loss 0.2594 (0.2461)	 acc 93.750 (89.187)
Test	  accuracy: 83.974 (Err: 0.442 )

Epoch: [15]	 Loss 0.1265 (0.2527)	 acc 93.750 (89.206)
Test	  accuracy: 80.449 (Err: 0.552 )

Epoch: [16]	 Loss 0.2755 (0.2448)	 acc 84.375 (89.417)
Test	  accuracy: 82.532 (Err: 0.565 )

Epoch: [17]	 Loss 0.1059 (0.2412)	 acc 96.875 (89.724)
Test	  accuracy: 83.173 (Err: 0.535 )

Epoch: [18]	 Loss 0.1674 (0.2414)	 acc 93.750 (89.935)
Test	  accuracy: 83.013 (Err: 0.482 )

Epoch: [19]	 Loss 0.1477 (0.2446)	 acc 96.875 (89.130)
Test	  accuracy: 80.929 (Err: 0.534 )

Epoch: [20]	 Loss 0.1657 (0.2343)	 acc 87.500 (90.184)
Test	  accuracy: 80.769 (Err: 0.557 )

Epoch: [21]	 Loss 0.1155 (0.2261)	 acc 93.750 (90.759)
Test	  accuracy: 80.929 (Err: 0.601 )

Epoch: [22]	 Loss 0.1032 (0.2301)	 acc 100.000 (90.548)
Test	  accuracy: 80.288 (Err: 0.593 )

Epoch: [23]	 Loss 0.1658 (0.2315)	 acc 96.875 (90.318)
Test	  accuracy: 80.288 (Err: 0.515 )

Epoch: [24]	 Loss 0.1505 (0.2343)	 acc 93.750 (90.318)
Test	  accuracy: 76.763 (Err: 0.559 )

Epoch: [25]	 Loss 0.2476 (0.2350)	 acc 90.625 (90.318)
Test	  accuracy: 81.731 (Err: 0.559 )

Epoch: [26]	 Loss 0.2010 (0.2262)	 acc 90.625 (90.510)
Test	  accuracy: 82.853 (Err: 0.438 )

Epoch: [27]	 Loss 0.1187 (0.2278)	 acc 96.875 (90.146)
Test	  accuracy: 83.173 (Err: 0.533 )

Epoch: [28]	 Loss 0.2322 (0.2230)	 acc 93.750 (90.932)
Test	  accuracy: 70.673 (Err: 0.825 )

Epoch: [29]	 Loss 0.3878 (0.2236)	 acc 81.250 (91.008)
Test	  accuracy: 82.532 (Err: 0.525 )

Epoch: [30]	 Loss 0.1424 (0.2226)	 acc 96.875 (90.491)
Test	  accuracy: 77.885 (Err: 0.648 )

Epoch: [31]	 Loss 0.2103 (0.2025)	 acc 87.500 (91.354)
Test	  accuracy: 77.564 (Err: 0.598 )

Epoch: [32]	 Loss 0.1071 (0.2135)	 acc 96.875 (91.085)
Test	  accuracy: 70.192 (Err: 0.935 )

Epoch: [33]	 Loss 0.4571 (0.2067)	 acc 84.375 (91.507)
Test	  accuracy: 78.205 (Err: 0.660 )

Epoch: [34]	 Loss 0.2174 (0.2031)	 acc 96.875 (91.852)
Test	  accuracy: 74.840 (Err: 0.599 )

Epoch: [35]	 Loss 0.1804 (0.2132)	 acc 93.750 (91.162)
Test	  accuracy: 80.288 (Err: 0.428 )

Epoch: [36]	 Loss 0.1647 (0.1928)	 acc 90.625 (92.427)
Test	  accuracy: 78.526 (Err: 0.602 )

Epoch: [37]	 Loss 0.2268 (0.1932)	 acc 90.625 (92.082)
Test	  accuracy: 77.885 (Err: 0.578 )

Epoch: [38]	 Loss 0.4932 (0.1998)	 acc 81.250 (91.986)
Test	  accuracy: 80.128 (Err: 0.541 )

Epoch: [39]	 Loss 0.2058 (0.2009)	 acc 93.750 (91.660)
Test	  accuracy: 81.731 (Err: 0.553 )

Epoch: [40]	 Loss 0.2011 (0.2073)	 acc 87.500 (91.660)
Test	  accuracy: 82.372 (Err: 0.561 )

Epoch: [41]	 Loss 0.0550 (0.2026)	 acc 100.000 (91.603)
Test	  accuracy: 81.731 (Err: 0.512 )

Epoch: [42]	 Loss 0.2505 (0.1959)	 acc 87.500 (92.140)
Test	  accuracy: 79.006 (Err: 0.704 )

Epoch: [43]	 Loss 0.2019 (0.1909)	 acc 90.625 (91.986)
Test	  accuracy: 75.801 (Err: 0.679 )

Epoch: [44]	 Loss 0.1630 (0.1885)	 acc 93.750 (92.465)
Test	  accuracy: 76.603 (Err: 0.663 )

Epoch: [45]	 Loss 0.1019 (0.1834)	 acc 96.875 (92.734)
Test	  accuracy: 76.442 (Err: 0.673 )

Epoch: [46]	 Loss 0.3616 (0.1725)	 acc 87.500 (92.772)
Test	  accuracy: 76.923 (Err: 0.588 )

Epoch: [47]	 Loss 0.2186 (0.1927)	 acc 90.625 (92.197)
Test	  accuracy: 80.769 (Err: 0.649 )

Epoch: [48]	 Loss 0.1960 (0.1783)	 acc 90.625 (92.983)
Test	  accuracy: 78.365 (Err: 0.695 )

Epoch: [49]	 Loss 0.1644 (0.1935)	 acc 90.625 (92.350)
Test	  accuracy: 77.083 (Err: 0.775 )

Epoch: [50]	 Loss 0.1913 (0.1817)	 acc 96.875 (92.926)
Test	  accuracy: 75.000 (Err: 0.839 )

Epoch: [51]	 Loss 0.1713 (0.1811)	 acc 90.625 (92.887)
Test	  accuracy: 75.160 (Err: 0.821 )

Epoch: [52]	 Loss 0.1238 (0.1767)	 acc 96.875 (93.232)
Test	  accuracy: 77.564 (Err: 0.731 )

Epoch: [53]	 Loss 0.1265 (0.1730)	 acc 90.625 (93.424)
Test	  accuracy: 74.359 (Err: 0.872 )

Epoch: [54]	 Loss 0.1241 (0.1704)	 acc 96.875 (93.328)
Test	  accuracy: 74.679 (Err: 0.985 )

Epoch: [55]	 Loss 0.0527 (0.1599)	 acc 100.000 (93.865)
Test	  accuracy: 75.160 (Err: 0.824 )

Epoch: [56]	 Loss 0.0798 (0.1642)	 acc 100.000 (93.903)
Test	  accuracy: 80.449 (Err: 0.671 )

Epoch: [57]	 Loss 0.1225 (0.1738)	 acc 93.750 (92.868)
Test	  accuracy: 76.282 (Err: 0.882 )

Epoch: [58]	 Loss 0.0949 (0.1700)	 acc 96.875 (93.654)
Test	  accuracy: 77.404 (Err: 0.662 )

Epoch: [59]	 Loss 0.0892 (0.1601)	 acc 100.000 (93.980)
Test	  accuracy: 79.968 (Err: 0.539 )

Epoch: [60]	 Loss 0.1156 (0.1733)	 acc 90.625 (93.271)
Test	  accuracy: 78.526 (Err: 0.684 )

Epoch: [61]	 Loss 0.1025 (0.1634)	 acc 100.000 (93.597)
Test	  accuracy: 77.564 (Err: 0.772 )

Epoch: [62]	 Loss 0.1584 (0.1572)	 acc 93.750 (94.383)
Test	  accuracy: 81.250 (Err: 0.630 )

Epoch: [63]	 Loss 0.1455 (0.1651)	 acc 93.750 (93.443)
Test	  accuracy: 76.763 (Err: 0.775 )

Epoch: [64]	 Loss 0.1207 (0.1518)	 acc 96.875 (94.057)
Test	  accuracy: 77.564 (Err: 0.858 )

Epoch: [65]	 Loss 0.2152 (0.1511)	 acc 93.750 (94.133)
Test	  accuracy: 76.442 (Err: 0.778 )

Epoch: [66]	 Loss 0.0791 (0.1594)	 acc 96.875 (93.520)
Test	  accuracy: 76.122 (Err: 0.794 )

Epoch: [67]	 Loss 0.1334 (0.1560)	 acc 96.875 (94.038)
Test	  accuracy: 77.244 (Err: 0.746 )

Epoch: [68]	 Loss 0.1571 (0.1512)	 acc 93.750 (94.268)
Test	  accuracy: 71.474 (Err: 1.190 )

Epoch: [69]	 Loss 0.3265 (0.1599)	 acc 90.625 (93.558)
Test	  accuracy: 77.564 (Err: 0.750 )

Epoch: [70]	 Loss 0.2705 (0.1747)	 acc 90.625 (93.002)
Test	  accuracy: 80.769 (Err: 0.589 )

Epoch: [71]	 Loss 0.1984 (0.1654)	 acc 93.750 (93.597)
Test	  accuracy: 80.769 (Err: 0.627 )

Epoch: [72]	 Loss 0.2339 (0.1527)	 acc 84.375 (93.712)
Test	  accuracy: 74.038 (Err: 0.996 )

Epoch: [73]	 Loss 0.1955 (0.1595)	 acc 87.500 (93.923)
Test	  accuracy: 75.801 (Err: 0.740 )

Epoch: [74]	 Loss 0.1115 (0.1520)	 acc 96.875 (94.191)
Test	  accuracy: 77.885 (Err: 0.738 )

Epoch: [75]	 Loss 0.0434 (0.1537)	 acc 96.875 (94.018)
Test	  accuracy: 76.763 (Err: 0.695 )

Epoch: [76]	 Loss 0.1612 (0.1471)	 acc 96.875 (94.038)
Test	  accuracy: 80.288 (Err: 0.570 )

Epoch: [77]	 Loss 0.0675 (0.1502)	 acc 96.875 (94.038)
Test	  accuracy: 75.321 (Err: 0.946 )

Epoch: [78]	 Loss 0.0364 (0.1571)	 acc 100.000 (93.731)
Test	  accuracy: 76.923 (Err: 0.791 )

Epoch: [79]	 Loss 0.1790 (0.1423)	 acc 90.625 (94.363)
Test	  accuracy: 75.801 (Err: 0.811 )

Epoch: [80]	 Loss 0.1887 (0.1543)	 acc 93.750 (93.635)
Test	  accuracy: 75.641 (Err: 0.958 )

Epoch: [81]	 Loss 0.0446 (0.1450)	 acc 100.000 (94.479)
Test	  accuracy: 79.167 (Err: 0.572 )

Epoch: [82]	 Loss 0.1777 (0.1423)	 acc 93.750 (94.555)
Test	  accuracy: 76.442 (Err: 0.881 )

Epoch: [83]	 Loss 0.2291 (0.1489)	 acc 90.625 (94.479)
Test	  accuracy: 81.090 (Err: 0.503 )

Epoch: [84]	 Loss 0.0627 (0.1425)	 acc 96.875 (94.459)
Test	  accuracy: 79.327 (Err: 0.607 )

Epoch: [85]	 Loss 0.0716 (0.1349)	 acc 100.000 (94.785)
Test	  accuracy: 78.045 (Err: 0.802 )

Epoch: [86]	 Loss 0.1440 (0.1421)	 acc 96.875 (94.229)
Test	  accuracy: 72.436 (Err: 1.164 )

Epoch: [87]	 Loss 0.0782 (0.1343)	 acc 96.875 (95.015)
Test	  accuracy: 77.083 (Err: 0.890 )

Epoch: [88]	 Loss 0.3209 (0.1376)	 acc 87.500 (94.728)
Test	  accuracy: 77.083 (Err: 0.804 )

Epoch: [89]	 Loss 0.1713 (0.1406)	 acc 93.750 (94.459)
Test	  accuracy: 78.045 (Err: 0.644 )

Epoch: [90]	 Loss 0.1226 (0.1389)	 acc 96.875 (94.440)
Test	  accuracy: 78.205 (Err: 0.731 )

Epoch: [91]	 Loss 0.0798 (0.1374)	 acc 93.750 (94.574)
Test	  accuracy: 76.603 (Err: 0.938 )

Epoch: [92]	 Loss 0.1950 (0.1425)	 acc 93.750 (94.191)
Test	  accuracy: 83.974 (Err: 0.472 )

Epoch: [93]	 Loss 0.1270 (0.1502)	 acc 93.750 (93.942)
Test	  accuracy: 78.686 (Err: 0.623 )

Epoch: [94]	 Loss 0.0521 (0.1389)	 acc 100.000 (94.574)
Test	  accuracy: 78.205 (Err: 0.698 )

Epoch: [95]	 Loss 0.0774 (0.1407)	 acc 96.875 (94.287)
Test	  accuracy: 74.199 (Err: 1.097 )

Epoch: [96]	 Loss 0.0780 (0.1273)	 acc 100.000 (95.303)
Test	  accuracy: 74.840 (Err: 1.030 )

Epoch: [97]	 Loss 0.1992 (0.1400)	 acc 87.500 (94.306)
Test	  accuracy: 73.878 (Err: 0.983 )

Epoch: [98]	 Loss 0.1553 (0.1301)	 acc 93.750 (94.824)
Test	  accuracy: 78.526 (Err: 0.790 )

Epoch: [99]	 Loss 0.1574 (0.1280)	 acc 93.750 (95.073)
Test	  accuracy: 82.372 (Err: 0.502 )

{'best_acc': 83.97435897435898, 'test_accs': [68.58974358974359, 71.9551280095027, 67.46794866904234, 67.30769225878593, 78.68589724027194, 79.48717909592848, 81.89102524977464, 78.8461537483411, 75.96153846153847, 83.49358954796425, 82.8525639069386, 83.81410256410257, 81.89102544540015, 82.8525641025641, 83.97435897435898, 80.44871775309245, 82.53205108642578, 83.17307692307692, 83.01282031719501, 80.92948698386168, 80.76923057360527, 80.92948708167442, 80.28846114721054, 80.28846124502329, 76.76282031719501, 81.73076883951823, 82.85256380912585, 83.17307692307692, 70.67307682526418, 82.53205108642578, 77.88461518898988, 77.56410256410257, 70.1923076923077, 78.20512810731546, 74.83974339411809, 80.28846114721054, 78.52564083001552, 77.88461538461539, 80.12820483476688, 81.73076893733098, 82.37179457835661, 81.73076883951823, 79.00641006078476, 75.80128185565655, 76.6025639069386, 76.44230749668219, 76.92307672745142, 80.76923037797977, 78.36538451757187, 77.08333323552058, 74.99999990218726, 75.1602562146309, 77.56410246628981, 74.35897416334886, 74.67948698386168, 75.1602562146309, 80.4487178509052, 76.28205128205128, 77.40384595822066, 79.96794842451047, 78.52564083001552, 77.56410236847707, 81.24999970656175, 76.76282031719501, 77.56410256410257, 76.4423076923077, 76.12179467616937, 77.24358954796425, 71.47435882763985, 77.56410246628981, 80.76923047579251, 80.76923047579251, 74.03846144064879, 75.80128185565655, 77.88461518898988, 76.76282041500777, 80.28846134283604, 75.32051262488731, 76.92307672745142, 75.8012819534693, 75.64102564102564, 79.16666656885391, 76.4423076923077, 81.08974329630534, 79.32692297911032, 78.0448717948718, 72.43589743589743, 77.08333323552058, 77.08333323552058, 78.04487169705905, 78.2051280095027, 76.6025641025641, 83.97435858310797, 78.68589733808469, 78.2051280095027, 74.19871775309245, 74.83974349193083, 73.87820512820512, 78.52564073220277, 82.37179467616937], 'test_losses': [0.7231159729835315, 0.6380702868486062, 0.6552564746294266, 0.6907517512639364, 0.4400800069173177, 0.43011183157945293, 0.4308960132109813, 0.4990290785447145, 0.8035798248572227, 0.4711791231082036, 0.4410359920599522, 0.4735146592824887, 0.5355223134542123, 0.42307401926089555, 0.44233905046414107, 0.5523317066522745, 0.5653851926326752, 0.5352113781831204, 0.48231326387478757, 0.5341178996440692, 0.5572010790690397, 0.6014561530871269, 0.5931602762295649, 0.5152645049951016, 0.5587590149579904, 0.5588462192278641, 0.43796729735839063, 0.5327883255787385, 0.8252261212238898, 0.5245115932745811, 0.6475116301041383, 0.5978267425910021, 0.9353964559924908, 0.6600197358773305, 0.5986608561032858, 0.4284693812712645, 0.6021840622027715, 0.5781203454885727, 0.5407838301780896, 0.5529339642096789, 0.5612998329676114, 0.5116002154655945, 0.7044730601020348, 0.6794585180588257, 0.6625178295832413, 0.6725528557331134, 0.5875925586009637, 0.6488062273233365, 0.6953055216715887, 0.7745170767108599, 0.8393946403685288, 0.8205824297590133, 0.73054033326797, 0.8720525149733592, 0.9846301401654879, 0.8244093022285364, 0.6707696333909646, 0.8820722570212988, 0.6624152937378639, 0.5392078581528786, 0.6844464953129108, 0.7715189965107502, 0.6300827906681941, 0.7751022943128378, 0.8582403600120392, 0.7784656735184865, 0.7935737617886983, 0.746241500075811, 1.1895322200770562, 0.7502081245183945, 0.5890851108691632, 0.626763600951586, 0.9963630320361028, 0.7395484180022509, 0.7379631874844049, 0.6945124455751517, 0.5704884670483761, 0.9455543187184211, 0.7912737078582629, 0.8107982006592628, 0.9582826605018897, 0.5724450716605554, 0.8810136795808108, 0.502648474696355, 0.6072656632615969, 0.8019427894017636, 1.1636315293801136, 0.8904552105575417, 0.8043607300959337, 0.6436774518627387, 0.7312633497401689, 0.9379574873795112, 0.4722891308558293, 0.6233867907371277, 0.6984090721951082, 1.0967647109944851, 1.030144792336684, 0.9832772073837427, 0.7903213942280183, 0.5022395027753634], 'train_acss': [72.7569018404908, 76.13113496932516, 77.85659509202453, 79.9463190184049, 81.25, 85.12269938650307, 85.79371165644172, 86.36886503067484, 87.82592024539878, 87.36579754601227, 87.67254601226993, 88.8228527607362, 88.01763803680981, 88.8611963190184, 89.18711656441718, 89.20628834355828, 89.41717791411043, 89.7239263803681, 89.93481595092024, 89.12960122699387, 90.1840490797546, 90.75920245398773, 90.54831288343559, 90.31825153374233, 90.31825153374233, 90.31825153374233, 90.50996932515338, 90.1457055214724, 90.93174846625767, 91.00843558282209, 90.49079754601227, 91.35352760736197, 91.0851226993865, 91.5069018404908, 91.85199386503068, 91.16180981595092, 92.4271472392638, 92.08205521472392, 91.9861963190184, 91.66027607361963, 91.66027607361963, 91.60276073619632, 92.13957055214723, 91.9861963190184, 92.46549079754601, 92.73389570552148, 92.77223926380368, 92.19708588957056, 92.98312883435582, 92.35046012269939, 92.92561349693251, 92.8872699386503, 93.23236196319019, 93.42407975460122, 93.3282208588957, 93.86503067484662, 93.90337423312883, 92.8680981595092, 93.65414110429448, 93.98006134969326, 93.2707055214724, 93.59662576687117, 94.38266871165644, 93.44325153374233, 94.05674846625767, 94.13343558282209, 93.51993865030674, 94.03757668711657, 94.26763803680981, 93.55828220858896, 93.00230061349693, 93.59662576687117, 93.71165644171779, 93.92254601226993, 94.1909509202454, 94.01840490797547, 94.03757668711657, 94.03757668711657, 93.7308282208589, 94.36349693251533, 93.63496932515338, 94.47852760736197, 94.55521472392638, 94.47852760736197, 94.45935582822086, 94.78527607361963, 94.2292944785276, 95.01533742331289, 94.72776073619632, 94.45935582822086, 94.44018404907976, 94.57438650306749, 94.1909509202454, 93.94171779141104, 94.57438650306749, 94.28680981595092, 95.30291411042944, 94.30598159509202, 94.82361963190183, 95.0728527607362], 'train_losses': [0.5555595027888479, 0.48174315435023396, 0.45634355501163226, 0.42219058794477965, 0.4029726726145832, 0.3327896781494281, 0.3156019259084222, 0.2975328065500669, 0.27207204763509013, 0.2879638222097619, 0.27852640890636327, 0.25324941577355553, 0.26948477245547287, 0.25957308396514206, 0.2460554821725272, 0.2526800390218665, 0.2447589583740644, 0.24121755402695183, 0.24137051970315127, 0.24459849992778404, 0.23430257633419857, 0.22607011462281817, 0.23010367863565867, 0.2314806668304958, 0.23426410034755987, 0.2350040100103507, 0.22619117348472034, 0.2278194982398507, 0.22299377345965685, 0.22362015327792958, 0.22260311136216474, 0.20248497309494604, 0.21348498455403042, 0.20667827421902146, 0.203130292746187, 0.21322162988727078, 0.19281893164102284, 0.1932460444470856, 0.19984451644808238, 0.20087688420813507, 0.20732780303691795, 0.20258505950950406, 0.19594544656803273, 0.190864335448464, 0.18851911052604395, 0.1833814778393763, 0.17251919593913423, 0.19274884690901983, 0.1782546134814163, 0.19349819129230056, 0.1817287121630885, 0.18112437792716582, 0.17669288713507858, 0.17298835854223168, 0.1704071433357666, 0.15991019368445947, 0.1642154004950465, 0.17379682757920284, 0.16995621129778996, 0.16008004253627334, 0.17329152943166487, 0.163366626011082, 0.15724985977989034, 0.16510045798095457, 0.15184668095016773, 0.1511455178717894, 0.1593582709234185, 0.1560270067281518, 0.15118509951544687, 0.15991252770453143, 0.17468913804534023, 0.16543072425514643, 0.15268409435003083, 0.1594982282058593, 0.1519504671722102, 0.153692990747149, 0.14706278272384515, 0.15023598020062126, 0.1571336591536282, 0.14228587204510448, 0.15427115728701551, 0.14500253270465904, 0.1422836073885666, 0.1489390153095035, 0.14248967216424416, 0.13489493160891386, 0.1420823096131986, 0.13431138399005668, 0.1375928003958032, 0.14056716598798893, 0.13892904595911867, 0.13738434030417285, 0.14252937918791742, 0.150163346775836, 0.13894428468189357, 0.1407109457839486, 0.12730648366311575, 0.1400378986827435, 0.13013990929613084, 0.12795716311576907]}
2022-01-01 16:44:27.068498



================================================================================



---------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 128, 32, 32]           1,152
              ReLU-2          [-1, 128, 32, 32]               0
         MaxPool2d-3          [-1, 128, 16, 16]               0
           Flatten-4             [-1, 128, 256]               0
         Tokenizer-5             [-1, 256, 128]               0
           Dropout-6             [-1, 256, 128]               0
         LayerNorm-7             [-1, 256, 128]             256
            Linear-8             [-1, 256, 384]          49,152
           Dropout-9          [-1, 4, 256, 256]               0
           Linear-10             [-1, 256, 128]          16,512
          Dropout-11             [-1, 256, 128]               0
        Attention-12             [-1, 256, 128]               0
         Identity-13             [-1, 256, 128]               0
        LayerNorm-14             [-1, 256, 128]             256
           Linear-15             [-1, 256, 512]          66,048
          Dropout-16             [-1, 256, 512]               0
           Linear-17             [-1, 256, 128]          65,664
          Dropout-18             [-1, 256, 128]               0
         Identity-19             [-1, 256, 128]               0
TransformerEncoderLayer-20             [-1, 256, 128]               0
        LayerNorm-21             [-1, 256, 128]             256
           Linear-22             [-1, 256, 384]          49,152
          Dropout-23          [-1, 4, 256, 256]               0
           Linear-24             [-1, 256, 128]          16,512
          Dropout-25             [-1, 256, 128]               0
        Attention-26             [-1, 256, 128]               0
         DropPath-27             [-1, 256, 128]               0
        LayerNorm-28             [-1, 256, 128]             256
           Linear-29             [-1, 256, 512]          66,048
          Dropout-30             [-1, 256, 512]               0
           Linear-31             [-1, 256, 128]          65,664
          Dropout-32             [-1, 256, 128]               0
         DropPath-33             [-1, 256, 128]               0
TransformerEncoderLayer-34             [-1, 256, 128]               0
        LayerNorm-35             [-1, 256, 128]             256
           Linear-36             [-1, 256, 384]          49,152
          Dropout-37          [-1, 4, 256, 256]               0
           Linear-38             [-1, 256, 128]          16,512
          Dropout-39             [-1, 256, 128]               0
        Attention-40             [-1, 256, 128]               0
         DropPath-41             [-1, 256, 128]               0
        LayerNorm-42             [-1, 256, 128]             256
           Linear-43             [-1, 256, 512]          66,048
          Dropout-44             [-1, 256, 512]               0
           Linear-45             [-1, 256, 128]          65,664
          Dropout-46             [-1, 256, 128]               0
         DropPath-47             [-1, 256, 128]               0
TransformerEncoderLayer-48             [-1, 256, 128]               0
        LayerNorm-49             [-1, 256, 128]             256
           Linear-50             [-1, 256, 384]          49,152
          Dropout-51          [-1, 4, 256, 256]               0
           Linear-52             [-1, 256, 128]          16,512
          Dropout-53             [-1, 256, 128]               0
        Attention-54             [-1, 256, 128]               0
         DropPath-55             [-1, 256, 128]               0
        LayerNorm-56             [-1, 256, 128]             256
           Linear-57             [-1, 256, 512]          66,048
          Dropout-58             [-1, 256, 512]               0
           Linear-59             [-1, 256, 128]          65,664
          Dropout-60             [-1, 256, 128]               0
         DropPath-61             [-1, 256, 128]               0
TransformerEncoderLayer-62             [-1, 256, 128]               0
        LayerNorm-63             [-1, 256, 128]             256
           Linear-64               [-1, 256, 1]             129
           Linear-65                    [-1, 2]             258
TransformerClassifier-66                    [-1, 2]               0
================================================================
Total params: 793,347
Trainable params: 793,347
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.02
Forward/backward pass size (MB): 32.25
Params size (MB): 3.03
Estimated Total Size (MB): 35.29
----------------------------------------------------------------
None
2022-01-01 19:02:46.446149
Epoch: [0]	 Loss 0.6086 (0.5501)	 acc 65.625 (73.620)
Test	  accuracy: 60.897 (Err: 0.744 )

Epoch: [1]	 Loss 0.4072 (0.5117)	 acc 81.250 (72.642)
Test	  accuracy: 59.776 (Err: 0.729 )

Epoch: [2]	 Loss 0.5627 (0.4740)	 acc 59.375 (74.176)
Test	  accuracy: 62.821 (Err: 0.708 )

Epoch: [3]	 Loss 0.4394 (0.4571)	 acc 65.625 (75.805)
Test	  accuracy: 68.910 (Err: 0.587 )

Epoch: [4]	 Loss 0.4754 (0.4336)	 acc 68.750 (77.646)
Test	  accuracy: 72.115 (Err: 0.553 )

Epoch: [5]	 Loss 0.4272 (0.4142)	 acc 84.375 (79.218)
Test	  accuracy: 78.526 (Err: 0.464 )

Epoch: [6]	 Loss 0.4768 (0.3765)	 acc 81.250 (81.653)
Test	  accuracy: 80.288 (Err: 0.473 )

Epoch: [7]	 Loss 0.5005 (0.3508)	 acc 84.375 (83.416)
Test	  accuracy: 82.692 (Err: 0.448 )

Epoch: [8]	 Loss 0.3783 (0.3386)	 acc 81.250 (83.857)
Test	  accuracy: 81.090 (Err: 0.409 )

Epoch: [9]	 Loss 0.2568 (0.3315)	 acc 90.625 (84.183)
Test	  accuracy: 82.692 (Err: 0.407 )

Epoch: [10]	 Loss 0.2180 (0.3176)	 acc 93.750 (85.909)
Test	  accuracy: 83.013 (Err: 0.417 )

Epoch: [11]	 Loss 0.3767 (0.3216)	 acc 81.250 (84.682)
Test	  accuracy: 78.526 (Err: 0.629 )

Epoch: [12]	 Loss 0.1284 (0.3093)	 acc 100.000 (85.966)
Test	  accuracy: 83.333 (Err: 0.407 )

Epoch: [13]	 Loss 0.4443 (0.2955)	 acc 81.250 (86.503)
Test	  accuracy: 82.853 (Err: 0.413 )

Epoch: [14]	 Loss 0.4579 (0.2912)	 acc 78.125 (86.848)
Test	  accuracy: 82.372 (Err: 0.483 )

Epoch: [15]	 Loss 0.1286 (0.2778)	 acc 93.750 (87.960)
Test	  accuracy: 83.013 (Err: 0.492 )

Epoch: [16]	 Loss 0.2447 (0.2740)	 acc 90.625 (87.692)
Test	  accuracy: 84.615 (Err: 0.449 )

Epoch: [17]	 Loss 0.2111 (0.2722)	 acc 90.625 (88.344)
Test	  accuracy: 83.974 (Err: 0.432 )

Epoch: [18]	 Loss 0.2957 (0.2887)	 acc 90.625 (86.580)
Test	  accuracy: 84.455 (Err: 0.401 )

Epoch: [19]	 Loss 0.4049 (0.2639)	 acc 84.375 (88.535)
Test	  accuracy: 84.295 (Err: 0.417 )

Epoch: [20]	 Loss 0.3247 (0.2575)	 acc 84.375 (88.919)
Test	  accuracy: 82.532 (Err: 0.514 )

Epoch: [21]	 Loss 0.2182 (0.2654)	 acc 93.750 (88.746)
Test	  accuracy: 82.372 (Err: 0.562 )

Epoch: [22]	 Loss 0.2484 (0.2618)	 acc 84.375 (88.382)
Test	  accuracy: 82.212 (Err: 0.524 )

Epoch: [23]	 Loss 0.1352 (0.2541)	 acc 96.875 (88.650)
Test	  accuracy: 82.692 (Err: 0.487 )

Epoch: [24]	 Loss 0.3547 (0.2490)	 acc 81.250 (89.302)
Test	  accuracy: 84.295 (Err: 0.428 )

Epoch: [25]	 Loss 0.1846 (0.2510)	 acc 90.625 (89.149)
Test	  accuracy: 84.135 (Err: 0.410 )

Epoch: [26]	 Loss 0.1787 (0.2517)	 acc 90.625 (89.245)
Test	  accuracy: 81.250 (Err: 0.612 )

Epoch: [27]	 Loss 0.1123 (0.2473)	 acc 96.875 (89.724)
Test	  accuracy: 85.737 (Err: 0.404 )

Epoch: [28]	 Loss 0.3244 (0.2554)	 acc 87.500 (89.340)
Test	  accuracy: 83.654 (Err: 0.426 )

Epoch: [29]	 Loss 0.2038 (0.2327)	 acc 87.500 (90.472)
Test	  accuracy: 82.372 (Err: 0.390 )

Epoch: [30]	 Loss 0.2215 (0.2366)	 acc 90.625 (90.088)
Test	  accuracy: 85.256 (Err: 0.438 )

Epoch: [31]	 Loss 0.2233 (0.2421)	 acc 90.625 (89.705)
Test	  accuracy: 84.455 (Err: 0.471 )

Epoch: [32]	 Loss 0.1544 (0.2377)	 acc 93.750 (89.858)
Test	  accuracy: 84.295 (Err: 0.387 )

Epoch: [33]	 Loss 0.1519 (0.2479)	 acc 90.625 (89.590)
Test	  accuracy: 85.256 (Err: 0.414 )

Epoch: [34]	 Loss 0.2727 (0.2405)	 acc 90.625 (90.031)
Test	  accuracy: 84.295 (Err: 0.504 )

Epoch: [35]	 Loss 0.4661 (0.2270)	 acc 75.000 (90.127)
Test	  accuracy: 85.417 (Err: 0.477 )

Epoch: [36]	 Loss 0.4101 (0.2302)	 acc 81.250 (90.242)
Test	  accuracy: 83.654 (Err: 0.494 )

Epoch: [37]	 Loss 0.2778 (0.2241)	 acc 87.500 (90.721)
Test	  accuracy: 82.051 (Err: 0.564 )

Epoch: [38]	 Loss 0.1654 (0.2240)	 acc 93.750 (90.567)
Test	  accuracy: 81.731 (Err: 0.652 )

Epoch: [39]	 Loss 0.1496 (0.2269)	 acc 93.750 (90.395)
Test	  accuracy: 82.692 (Err: 0.568 )

Epoch: [40]	 Loss 0.1223 (0.2272)	 acc 96.875 (90.587)
Test	  accuracy: 82.212 (Err: 0.540 )

Epoch: [41]	 Loss 0.1439 (0.2305)	 acc 96.875 (90.376)
Test	  accuracy: 82.853 (Err: 0.565 )

Epoch: [42]	 Loss 0.2124 (0.2168)	 acc 90.625 (91.296)
Test	  accuracy: 83.654 (Err: 0.557 )

Epoch: [43]	 Loss 0.1770 (0.2257)	 acc 87.500 (90.433)
Test	  accuracy: 82.853 (Err: 0.511 )

Epoch: [44]	 Loss 0.3457 (0.2279)	 acc 81.250 (90.203)
Test	  accuracy: 85.096 (Err: 0.415 )

Epoch: [45]	 Loss 0.2149 (0.2209)	 acc 90.625 (90.587)
Test	  accuracy: 83.654 (Err: 0.531 )

Epoch: [46]	 Loss 0.2136 (0.2215)	 acc 93.750 (90.587)
Test	  accuracy: 83.173 (Err: 0.525 )

Epoch: [47]	 Loss 0.2023 (0.2121)	 acc 93.750 (91.143)
Test	  accuracy: 82.692 (Err: 0.559 )

Epoch: [48]	 Loss 0.2606 (0.2178)	 acc 90.625 (90.491)
Test	  accuracy: 81.731 (Err: 0.548 )

Epoch: [49]	 Loss 0.2692 (0.2130)	 acc 84.375 (91.008)
Test	  accuracy: 83.013 (Err: 0.565 )

Epoch: [50]	 Loss 0.2525 (0.2092)	 acc 90.625 (91.392)
Test	  accuracy: 82.051 (Err: 0.631 )

Epoch: [51]	 Loss 0.1386 (0.2123)	 acc 90.625 (91.066)
Test	  accuracy: 79.167 (Err: 0.764 )

Epoch: [52]	 Loss 0.1877 (0.2194)	 acc 96.875 (90.893)
Test	  accuracy: 81.410 (Err: 0.582 )

Epoch: [53]	 Loss 0.1120 (0.2058)	 acc 93.750 (91.545)
Test	  accuracy: 82.532 (Err: 0.541 )

Epoch: [54]	 Loss 0.2090 (0.2073)	 acc 90.625 (91.430)
Test	  accuracy: 80.609 (Err: 0.631 )

Epoch: [55]	 Loss 0.2298 (0.2113)	 acc 87.500 (91.219)
Test	  accuracy: 83.013 (Err: 0.512 )

Epoch: [56]	 Loss 0.1533 (0.2091)	 acc 93.750 (91.718)
Test	  accuracy: 86.058 (Err: 0.411 )

Epoch: [57]	 Loss 0.0868 (0.2008)	 acc 96.875 (91.871)
Test	  accuracy: 81.410 (Err: 0.659 )

Epoch: [58]	 Loss 0.1037 (0.2094)	 acc 96.875 (91.564)
Test	  accuracy: 84.295 (Err: 0.453 )

Epoch: [59]	 Loss 0.1441 (0.2017)	 acc 90.625 (91.488)
Test	  accuracy: 81.571 (Err: 0.571 )

Epoch: [60]	 Loss 0.2993 (0.2064)	 acc 84.375 (91.238)
Test	  accuracy: 82.532 (Err: 0.562 )

Epoch: [61]	 Loss 0.1497 (0.2031)	 acc 93.750 (91.718)
Test	  accuracy: 79.167 (Err: 0.731 )

Epoch: [62]	 Loss 0.1576 (0.2013)	 acc 93.750 (91.948)
Test	  accuracy: 82.051 (Err: 0.576 )

Epoch: [63]	 Loss 0.1047 (0.1895)	 acc 96.875 (92.255)
Test	  accuracy: 83.013 (Err: 0.553 )

Epoch: [64]	 Loss 0.2740 (0.1916)	 acc 93.750 (92.216)
Test	  accuracy: 81.250 (Err: 0.653 )

Epoch: [65]	 Loss 0.1606 (0.2025)	 acc 90.625 (91.392)
Test	  accuracy: 82.212 (Err: 0.563 )

Epoch: [66]	 Loss 0.3110 (0.2006)	 acc 81.250 (91.526)
Test	  accuracy: 81.250 (Err: 0.697 )

Epoch: [67]	 Loss 0.1468 (0.2035)	 acc 93.750 (91.449)
Test	  accuracy: 81.090 (Err: 0.662 )

Epoch: [68]	 Loss 0.1177 (0.1936)	 acc 93.750 (92.427)
Test	  accuracy: 81.891 (Err: 0.536 )

Epoch: [69]	 Loss 0.1506 (0.2066)	 acc 93.750 (91.219)
Test	  accuracy: 81.731 (Err: 0.587 )

Epoch: [70]	 Loss 0.1226 (0.1875)	 acc 93.750 (92.216)
Test	  accuracy: 80.929 (Err: 0.744 )

Epoch: [71]	 Loss 0.1467 (0.1939)	 acc 93.750 (91.814)
Test	  accuracy: 80.769 (Err: 0.673 )

Epoch: [72]	 Loss 0.2462 (0.1899)	 acc 87.500 (92.235)
Test	  accuracy: 81.410 (Err: 0.632 )

Epoch: [73]	 Loss 0.2638 (0.1942)	 acc 93.750 (92.120)
Test	  accuracy: 80.929 (Err: 0.628 )

Epoch: [74]	 Loss 0.1939 (0.1941)	 acc 90.625 (92.197)
Test	  accuracy: 74.679 (Err: 0.835 )

Epoch: [75]	 Loss 0.2843 (0.1948)	 acc 90.625 (91.584)
Test	  accuracy: 80.609 (Err: 0.679 )

Epoch: [76]	 Loss 0.2142 (0.1832)	 acc 87.500 (92.696)
Test	  accuracy: 81.250 (Err: 0.634 )

Epoch: [77]	 Loss 0.1642 (0.1823)	 acc 93.750 (92.772)
Test	  accuracy: 82.212 (Err: 0.578 )

Epoch: [78]	 Loss 0.1991 (0.1943)	 acc 93.750 (92.370)
Test	  accuracy: 80.929 (Err: 0.655 )

Epoch: [79]	 Loss 0.2254 (0.1868)	 acc 93.750 (92.446)
Test	  accuracy: 81.410 (Err: 0.612 )

Epoch: [80]	 Loss 0.1656 (0.1986)	 acc 96.875 (91.890)
Test	  accuracy: 84.615 (Err: 0.413 )

Epoch: [81]	 Loss 0.0835 (0.1790)	 acc 96.875 (92.887)
Test	  accuracy: 81.410 (Err: 0.626 )

Epoch: [82]	 Loss 0.0680 (0.1788)	 acc 96.875 (92.964)
Test	  accuracy: 80.929 (Err: 0.640 )

Epoch: [83]	 Loss 0.2517 (0.1750)	 acc 90.625 (92.849)
Test	  accuracy: 80.929 (Err: 0.677 )

Epoch: [84]	 Loss 0.0866 (0.1810)	 acc 96.875 (92.657)
Test	  accuracy: 82.051 (Err: 0.553 )

Epoch: [85]	 Loss 0.1739 (0.1951)	 acc 90.625 (91.967)
Test	  accuracy: 79.647 (Err: 0.728 )

Epoch: [86]	 Loss 0.1169 (0.1901)	 acc 90.625 (92.005)
Test	  accuracy: 81.891 (Err: 0.649 )

Epoch: [87]	 Loss 0.2462 (0.1790)	 acc 90.625 (92.887)
Test	  accuracy: 83.173 (Err: 0.579 )

Epoch: [88]	 Loss 0.2273 (0.1979)	 acc 90.625 (91.948)
Test	  accuracy: 79.327 (Err: 0.726 )

Epoch: [89]	 Loss 0.2192 (0.1870)	 acc 90.625 (92.331)
Test	  accuracy: 80.929 (Err: 0.654 )

Epoch: [90]	 Loss 0.1391 (0.1817)	 acc 96.875 (92.619)
Test	  accuracy: 82.692 (Err: 0.603 )

Epoch: [91]	 Loss 0.2833 (0.1851)	 acc 87.500 (92.255)
Test	  accuracy: 83.814 (Err: 0.469 )

Epoch: [92]	 Loss 0.1427 (0.1852)	 acc 96.875 (92.542)
Test	  accuracy: 83.013 (Err: 0.610 )

Epoch: [93]	 Loss 0.1700 (0.1810)	 acc 90.625 (92.619)
Test	  accuracy: 81.731 (Err: 0.586 )

Epoch: [94]	 Loss 0.1408 (0.1792)	 acc 87.500 (93.079)
Test	  accuracy: 79.968 (Err: 0.727 )

Epoch: [95]	 Loss 0.0843 (0.1730)	 acc 96.875 (93.041)
Test	  accuracy: 81.571 (Err: 0.668 )

Epoch: [96]	 Loss 0.2782 (0.1770)	 acc 90.625 (92.753)
Test	  accuracy: 82.372 (Err: 0.589 )

Epoch: [97]	 Loss 0.2763 (0.1743)	 acc 81.250 (92.926)
Test	  accuracy: 83.013 (Err: 0.528 )

Epoch: [98]	 Loss 0.1619 (0.1770)	 acc 96.875 (92.715)
Test	  accuracy: 81.571 (Err: 0.656 )

Epoch: [99]	 Loss 0.1447 (0.1731)	 acc 93.750 (93.021)
Test	  accuracy: 82.853 (Err: 0.584 )

{'best_acc': 86.0576921120668, 'test_accs': [60.8974358913226, 59.77564097673465, 62.82051282051282, 68.9102562146309, 72.11538451757187, 78.52564102564102, 80.28846134283604, 82.69230730105669, 81.08974339411809, 82.69230749668219, 83.01282031719501, 78.52564063439003, 83.33333313770784, 82.8525639069386, 82.37179487179488, 83.01282051282051, 84.61538461538461, 83.97435877873347, 84.4551280095027, 84.2948717948718, 82.53205089080028, 82.37179467616937, 82.21153826591296, 82.69230730105669, 84.29487159924629, 84.13461538461539, 81.2499998043745, 85.73717909592848, 83.65384576259515, 82.37179467616937, 85.25641006078476, 84.4551282051282, 84.29487140362079, 85.25641006078476, 84.2948717948718, 85.41666627541566, 83.65384615384616, 82.05128185565655, 81.73076923076923, 82.69230749668219, 82.21153826591296, 82.8525641025641, 83.65384615384616, 82.8525641025641, 85.09615365052835, 83.65384615384616, 83.17307692307692, 82.69230749668219, 81.73076883951823, 83.01282051282051, 82.05128166003105, 79.16666637322841, 81.4102560190054, 82.53205108642578, 80.6089740655361, 83.01282031719501, 86.0576921120668, 81.4102560190054, 84.29487159924629, 81.57051242926183, 82.53205108642578, 79.16666637322841, 82.05128185565655, 83.01282031719501, 81.24999970656175, 82.21153807028746, 81.24999970656175, 81.08974339411809, 81.89102544540015, 81.73076883951823, 80.92948698386168, 80.76923067141801, 81.4102560190054, 80.92948698386168, 74.67948698386168, 80.6089742611616, 81.24999970656175, 82.21153807028746, 80.92948688604893, 81.41025611681816, 84.61538422413362, 81.41025611681816, 80.92948688604893, 80.92948688604893, 82.0512817578438, 79.6474357018104, 81.89102534758739, 83.17307653182593, 79.32692288129758, 80.92948688604893, 82.69230749668219, 83.81410256410257, 83.0128201215695, 81.73076903514372, 79.96794862013597, 81.57051262488731, 82.37179457835661, 83.01282021938226, 81.57051252707457, 82.85256380912585], 'test_losses': [0.7437730018909161, 0.7292211376703702, 0.7080987111116067, 0.5865024924278259, 0.553335978434636, 0.46378417962636703, 0.4733324341284923, 0.4476207754550836, 0.40874571601549786, 0.40743100566741747, 0.41710049372452956, 0.6285357673962911, 0.4069792200357486, 0.41290318430998385, 0.4829206619507227, 0.4920131525932214, 0.44876639965253, 0.4320779289954748, 0.4007765635465964, 0.41725867986679077, 0.5139273214034545, 0.5622790669783567, 0.5239260143194443, 0.4874729735729022, 0.42796232455816025, 0.4103594101392306, 0.6122000561310694, 0.40408058044237966, 0.42614418497452367, 0.3898920630797362, 0.43845379505402005, 0.4709934829137264, 0.38668087201240736, 0.41366203625996906, 0.5036006653920199, 0.47652580111454695, 0.494481487151904, 0.564076868387369, 0.652345556097153, 0.5678708981244992, 0.5404030711222918, 0.5652325054009756, 0.5570285488397647, 0.5108703986192361, 0.4146143610660846, 0.5306102847441648, 0.5254994340431995, 0.5589600373537112, 0.5484659465459677, 0.5645380616188049, 0.6305823387243809, 0.7640884891152382, 0.5816376923750608, 0.5409214282647158, 0.6308803906043371, 0.5120514730612437, 0.41102535525957745, 0.6589484990407259, 0.45318650817259765, 0.5710382014513016, 0.5622240965947126, 0.7306807417518053, 0.5755771711850778, 0.5532710200701004, 0.6532303778788983, 0.5631440465266888, 0.6968390861382852, 0.6621736482932017, 0.535772565083626, 0.5871249788846725, 0.7439240263058589, 0.6732406642956611, 0.6322910625201005, 0.6276460083631369, 0.8353546541661788, 0.6790920858963941, 0.633577864139508, 0.5784594332560514, 0.6547618424281095, 0.6116006951301526, 0.41273850890306324, 0.6256763488054276, 0.6399996070525585, 0.6773947053230726, 0.5532435339230758, 0.7279774363224323, 0.6491089482338, 0.5786915964041001, 0.7262138506540885, 0.6541485614501513, 0.6034560589454113, 0.46934702151860946, 0.6102278339557159, 0.5860889217792413, 0.7273550694569563, 0.6677852991299752, 0.589113787580759, 0.5282922486464182, 0.6555980359896635, 0.5836821496486664], 'train_acss': [73.61963190184049, 72.64187116564418, 74.17561349693251, 75.80521472392638, 77.6457055214724, 79.21779141104294, 81.65260736196319, 83.41641104294479, 83.85736196319019, 84.18328220858896, 85.90874233128834, 84.68174846625767, 85.96625766871166, 86.50306748466258, 86.84815950920246, 87.9601226993865, 87.69171779141104, 88.34355828220859, 86.579754601227, 88.53527607361963, 88.91871165644172, 88.74616564417178, 88.3819018404908, 88.65030674846626, 89.3021472392638, 89.14877300613497, 89.24463190184049, 89.7239263803681, 89.34049079754601, 90.47162576687117, 90.08819018404908, 89.704754601227, 89.85812883435582, 89.58972392638037, 90.03067484662577, 90.12653374233129, 90.24156441717791, 90.72085889570552, 90.56748466257669, 90.39493865030674, 90.58665644171779, 90.37576687116564, 91.29601226993866, 90.43328220858896, 90.2032208588957, 90.58665644171779, 90.58665644171779, 91.14263803680981, 90.49079754601227, 91.00843558282209, 91.39187116564418, 91.0659509202454, 90.89340490797547, 91.545245398773, 91.43021472392638, 91.21932515337423, 91.71779141104294, 91.87116564417178, 91.56441717791411, 91.4877300613497, 91.23849693251533, 91.71779141104294, 91.9478527607362, 92.25460122699387, 92.21625766871166, 91.39187116564418, 91.5260736196319, 91.44938650306749, 92.4271472392638, 91.21932515337423, 92.21625766871166, 91.81365030674847, 92.23542944785277, 92.12039877300613, 92.19708588957056, 91.58358895705521, 92.69555214723927, 92.77223926380368, 92.36963190184049, 92.4463190184049, 91.89033742331289, 92.8872699386503, 92.96395705521472, 92.8489263803681, 92.65720858895706, 91.9670245398773, 92.00536809815951, 92.8872699386503, 91.9478527607362, 92.33128834355828, 92.61886503067484, 92.25460122699387, 92.54217791411043, 92.61886503067484, 93.07898773006134, 93.04064417177914, 92.75306748466258, 92.92561349693251, 92.71472392638037, 93.02147239263803], 'train_losses': [0.5501465318393122, 0.5117330995439752, 0.4740015008698212, 0.4570881327602761, 0.43364792952508285, 0.41416362476495144, 0.37645383828256757, 0.35079014703539985, 0.3385679259256351, 0.331495453974952, 0.31764786713328097, 0.3215876938375227, 0.309319867038288, 0.2955308053756784, 0.2912165091081631, 0.27776272869183244, 0.27403903089783677, 0.27223395716193266, 0.2886843332118052, 0.26392753998194735, 0.25750081414825343, 0.2653833037139448, 0.26175816338852137, 0.2540728084148805, 0.24903128218431414, 0.2510095145439078, 0.25173387799891955, 0.24731409142902291, 0.25543262046173304, 0.23266732080216787, 0.23660152373504054, 0.24210525848017148, 0.2376839556942688, 0.24794674047663168, 0.24054223152757423, 0.22695199934982815, 0.23022168216529798, 0.22408830456755643, 0.22403026175645233, 0.2268780619820203, 0.2271762363658361, 0.23045794263573513, 0.2167773361952027, 0.22568363728333105, 0.2279137491814198, 0.22088531116766433, 0.2214960552797727, 0.212128597351671, 0.21782722703518312, 0.21296377224059193, 0.20917543686606402, 0.21231012887384262, 0.21939911259098288, 0.20583810200171967, 0.20732978562270205, 0.2113072969065122, 0.2090903045209639, 0.2007734964778818, 0.20944438001678034, 0.20167357040329215, 0.20642145965362618, 0.20313295127789668, 0.20127943468971488, 0.18952903576606622, 0.1915819412725835, 0.20250802756818526, 0.20060331708083123, 0.20348702106007768, 0.19361287204582997, 0.20662794796967068, 0.18752626722758534, 0.19385717621793044, 0.18986171849666197, 0.19415818017684608, 0.19408559287252602, 0.1948407705576142, 0.1831771777999913, 0.18233790152643356, 0.1943442227284601, 0.1868079398855841, 0.19862767441506765, 0.17897635940576623, 0.17884675349377416, 0.17503756111019228, 0.18099289698286292, 0.19514298201338645, 0.1900535730496506, 0.17900233592358103, 0.19790401935943067, 0.18702868257929212, 0.18166412010880337, 0.18506816422646763, 0.18518206326325246, 0.18101902552908916, 0.17916745866186048, 0.17298873335671572, 0.17700515568622052, 0.17432081553102272, 0.17704603976450084, 0.1730744101884175]}
2022-01-01 19:29:23.389162




model = CCT(
        img_size=IMG_SIZE,
        embedding_dim=128,
        n_input_channels=1,
        n_conv_layers=1,
        kernel_size=3,
        stride=2,
        padding=1,
        pooling_kernel_size=3,
        pooling_stride=2,
        pooling_padding=1,
        num_layers=4,
        num_heads=4,
        mlp_radio=1.,
        num_classes=2,
        positional_embedding='none', # ['sine', 'learnable', 'none']
        )
        
        
        ===========================================
        
        model = CCT(
        img_size=IMG_SIZE,
        embedding_dim=128,
        n_input_channels=1,
        n_conv_layers=1,
        kernel_size=3,
        stride=2,
        padding=1,
        pooling_kernel_size=3,
        pooling_stride=2,
        pooling_padding=1,
        num_layers=4,
        num_heads=4,
        mlp_radio=1.,
        num_classes=2,
        positional_embedding='none', # ['sine', 'learnable', 'none']
        )
print(summary(model.to(device), (1,IMG_SIZE ,IMG_SIZE)))


train_loader = DataLoader(CustomImageDataset('train.csv', '.', special_transform=transforms.Compose([tv.transforms.Grayscale(num_output_channels=1)]), transform=transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomVerticalFlip(), transforms.RandomRotation(.05)])), batch_size=128, shuffle=True, num_workers=2, pin_memory=True)

test_loader = DataLoader(CustomImageDataset('test.csv', '.', special_transform=transforms.Compose([tv.transforms.Grayscale(num_output_channels=1)])), batch_size=128, shuffle=False, num_workers=2, pin_memory=True)

        
        
        cuda
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1          [-1, 128, 32, 32]           1,152
              ReLU-2          [-1, 128, 32, 32]               0
         MaxPool2d-3          [-1, 128, 16, 16]               0
           Flatten-4             [-1, 128, 256]               0
         Tokenizer-5             [-1, 256, 128]               0
           Dropout-6             [-1, 256, 128]               0
         LayerNorm-7             [-1, 256, 128]             256
            Linear-8             [-1, 256, 384]          49,152
           Dropout-9          [-1, 4, 256, 256]               0
           Linear-10             [-1, 256, 128]          16,512
          Dropout-11             [-1, 256, 128]               0
        Attention-12             [-1, 256, 128]               0
         Identity-13             [-1, 256, 128]               0
        LayerNorm-14             [-1, 256, 128]             256
           Linear-15             [-1, 256, 512]          66,048
          Dropout-16             [-1, 256, 512]               0
           Linear-17             [-1, 256, 128]          65,664
          Dropout-18             [-1, 256, 128]               0
         Identity-19             [-1, 256, 128]               0
TransformerEncoderLayer-20             [-1, 256, 128]               0
        LayerNorm-21             [-1, 256, 128]             256
           Linear-22             [-1, 256, 384]          49,152
          Dropout-23          [-1, 4, 256, 256]               0
           Linear-24             [-1, 256, 128]          16,512
          Dropout-25             [-1, 256, 128]               0
        Attention-26             [-1, 256, 128]               0
         DropPath-27             [-1, 256, 128]               0
        LayerNorm-28             [-1, 256, 128]             256
           Linear-29             [-1, 256, 512]          66,048
          Dropout-30             [-1, 256, 512]               0
           Linear-31             [-1, 256, 128]          65,664
          Dropout-32             [-1, 256, 128]               0
         DropPath-33             [-1, 256, 128]               0
TransformerEncoderLayer-34             [-1, 256, 128]               0
        LayerNorm-35             [-1, 256, 128]             256
           Linear-36             [-1, 256, 384]          49,152
          Dropout-37          [-1, 4, 256, 256]               0
           Linear-38             [-1, 256, 128]          16,512
          Dropout-39             [-1, 256, 128]               0
        Attention-40             [-1, 256, 128]               0
         DropPath-41             [-1, 256, 128]               0
        LayerNorm-42             [-1, 256, 128]             256
           Linear-43             [-1, 256, 512]          66,048
          Dropout-44             [-1, 256, 512]               0
           Linear-45             [-1, 256, 128]          65,664
          Dropout-46             [-1, 256, 128]               0
         DropPath-47             [-1, 256, 128]               0
TransformerEncoderLayer-48             [-1, 256, 128]               0
        LayerNorm-49             [-1, 256, 128]             256
           Linear-50             [-1, 256, 384]          49,152
          Dropout-51          [-1, 4, 256, 256]               0
           Linear-52             [-1, 256, 128]          16,512
          Dropout-53             [-1, 256, 128]               0
        Attention-54             [-1, 256, 128]               0
         DropPath-55             [-1, 256, 128]               0
        LayerNorm-56             [-1, 256, 128]             256
           Linear-57             [-1, 256, 512]          66,048
          Dropout-58             [-1, 256, 512]               0
           Linear-59             [-1, 256, 128]          65,664
          Dropout-60             [-1, 256, 128]               0
         DropPath-61             [-1, 256, 128]               0
TransformerEncoderLayer-62             [-1, 256, 128]               0
        LayerNorm-63             [-1, 256, 128]             256
           Linear-64               [-1, 256, 1]             129
           Linear-65                    [-1, 2]             258
TransformerClassifier-66                    [-1, 2]               0
================================================================
Total params: 793,347
Trainable params: 793,347
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.02
Forward/backward pass size (MB): 32.25
Params size (MB): 3.03
Estimated Total Size (MB): 35.29
----------------------------------------------------------------
None
2022-01-01 21:04:30.690838
Epoch: [0]	 Loss 0.4814 (0.5435)	 acc 77.083 (73.121)
Test	  accuracy: 62.660 (Err: 0.682 )

Epoch: [1]	 Loss 0.5380 (0.4730)	 acc 71.875 (74.655)
Test	  accuracy: 63.462 (Err: 0.674 )

Epoch: [2]	 Loss 0.4471 (0.4575)	 acc 79.167 (76.668)
Test	  accuracy: 64.103 (Err: 0.692 )

Epoch: [3]	 Loss 0.3939 (0.4422)	 acc 75.000 (78.029)
Test	  accuracy: 69.391 (Err: 0.667 )

Epoch: [4]	 Loss 0.3966 (0.4082)	 acc 78.125 (80.272)
Test	  accuracy: 66.987 (Err: 0.608 )

Epoch: [5]	 Loss 0.4609 (0.3800)	 acc 73.958 (82.228)
Test	  accuracy: 76.923 (Err: 0.451 )

Epoch: [6]	 Loss 0.4208 (0.3370)	 acc 80.208 (84.317)
Test	  accuracy: 80.449 (Err: 0.425 )

Epoch: [7]	 Loss 0.3402 (0.3285)	 acc 82.292 (85.314)
Test	  accuracy: 75.160 (Err: 0.622 )

Epoch: [8]	 Loss 0.2449 (0.3073)	 acc 90.625 (86.196)
Test	  accuracy: 80.929 (Err: 0.451 )

Epoch: [9]	 Loss 0.2069 (0.2962)	 acc 91.667 (86.541)
Test	  accuracy: 81.090 (Err: 0.433 )

Epoch: [10]	 Loss 0.2677 (0.3291)	 acc 90.625 (84.835)
Test	  accuracy: 80.288 (Err: 0.491 )

Epoch: [11]	 Loss 0.3154 (0.2853)	 acc 87.500 (87.404)
Test	  accuracy: 80.449 (Err: 0.523 )

Epoch: [12]	 Loss 0.2695 (0.2645)	 acc 87.500 (88.516)
Test	  accuracy: 79.968 (Err: 0.527 )

Epoch: [13]	 Loss 0.3782 (0.2656)	 acc 85.417 (88.286)
Test	  accuracy: 80.769 (Err: 0.544 )

Epoch: [14]	 Loss 0.2480 (0.2765)	 acc 90.625 (87.653)
Test	  accuracy: 80.128 (Err: 0.514 )

Epoch: [15]	 Loss 0.2555 (0.2574)	 acc 84.375 (89.168)
Test	  accuracy: 80.449 (Err: 0.471 )

Epoch: [16]	 Loss 0.2726 (0.2444)	 acc 88.542 (89.666)
Test	  accuracy: 83.333 (Err: 0.386 )

Epoch: [17]	 Loss 0.2555 (0.2534)	 acc 89.583 (89.264)
Test	  accuracy: 79.327 (Err: 0.591 )

Epoch: [18]	 Loss 0.2467 (0.2580)	 acc 89.583 (88.785)
Test	  accuracy: 81.090 (Err: 0.550 )

Epoch: [19]	 Loss 0.2158 (0.2471)	 acc 91.667 (89.647)
Test	  accuracy: 79.006 (Err: 0.592 )

Epoch: [20]	 Loss 0.2476 (0.2411)	 acc 91.667 (89.666)
Test	  accuracy: 82.051 (Err: 0.508 )

Epoch: [21]	 Loss 0.2299 (0.2390)	 acc 92.708 (89.992)
Test	  accuracy: 82.532 (Err: 0.450 )

Epoch: [22]	 Loss 0.2726 (0.2397)	 acc 91.667 (90.088)
Test	  accuracy: 79.327 (Err: 0.724 )

Epoch: [23]	 Loss 0.1948 (0.2392)	 acc 92.708 (90.242)
Test	  accuracy: 82.532 (Err: 0.455 )

Epoch: [24]	 Loss 0.2284 (0.2211)	 acc 92.708 (90.836)
Test	  accuracy: 79.968 (Err: 0.642 )

Epoch: [25]	 Loss 0.0979 (0.2363)	 acc 95.833 (90.222)
Test	  accuracy: 80.449 (Err: 0.570 )

Epoch: [26]	 Loss 0.1807 (0.2366)	 acc 93.750 (90.203)
Test	  accuracy: 79.647 (Err: 0.536 )

Epoch: [27]	 Loss 0.1738 (0.2272)	 acc 92.708 (90.472)
Test	  accuracy: 81.571 (Err: 0.527 )

Epoch: [28]	 Loss 0.2207 (0.2216)	 acc 86.458 (90.759)
Test	  accuracy: 81.410 (Err: 0.550 )

Epoch: [29]	 Loss 0.1776 (0.2146)	 acc 93.750 (90.817)
Test	  accuracy: 80.609 (Err: 0.615 )

Epoch: [30]	 Loss 0.0847 (0.2089)	 acc 96.875 (91.334)
Test	  accuracy: 79.968 (Err: 0.634 )

Epoch: [31]	 Loss 0.2137 (0.2094)	 acc 91.667 (91.373)
Test	  accuracy: 78.365 (Err: 0.681 )

Epoch: [32]	 Loss 0.3601 (0.2056)	 acc 85.417 (91.737)
Test	  accuracy: 80.929 (Err: 0.587 )

Epoch: [33]	 Loss 0.1394 (0.2236)	 acc 95.833 (91.104)
Test	  accuracy: 82.853 (Err: 0.434 )

Epoch: [34]	 Loss 0.2149 (0.2057)	 acc 91.667 (91.603)
Test	  accuracy: 85.417 (Err: 0.379 )

Epoch: [35]	 Loss 0.1123 (0.2135)	 acc 95.833 (91.373)
Test	  accuracy: 82.532 (Err: 0.461 )

Epoch: [36]	 Loss 0.1762 (0.2200)	 acc 91.667 (91.028)
Test	  accuracy: 85.096 (Err: 0.369 )

Epoch: [37]	 Loss 0.1055 (0.2051)	 acc 95.833 (91.584)
Test	  accuracy: 71.955 (Err: 0.907 )

Epoch: [38]	 Loss 0.2112 (0.1956)	 acc 91.667 (91.679)
Test	  accuracy: 83.494 (Err: 0.411 )

Epoch: [39]	 Loss 0.3064 (0.2131)	 acc 86.458 (91.315)
Test	  accuracy: 74.038 (Err: 0.900 )

Epoch: [40]	 Loss 0.1614 (0.1915)	 acc 94.792 (92.370)
Test	  accuracy: 76.763 (Err: 0.683 )

Epoch: [41]	 Loss 0.2440 (0.1834)	 acc 90.625 (92.811)
Test	  accuracy: 71.314 (Err: 0.735 )

Epoch: [42]	 Loss 0.1450 (0.1839)	 acc 94.792 (92.753)
Test	  accuracy: 78.526 (Err: 0.620 )

Epoch: [43]	 Loss 0.2074 (0.1931)	 acc 92.708 (92.101)
Test	  accuracy: 77.885 (Err: 0.651 )

Epoch: [44]	 Loss 0.1928 (0.1818)	 acc 89.583 (92.945)
Test	  accuracy: 78.045 (Err: 0.593 )

Epoch: [45]	 Loss 0.2077 (0.1883)	 acc 91.667 (92.485)
Test	  accuracy: 83.974 (Err: 0.469 )

Epoch: [46]	 Loss 0.2312 (0.1832)	 acc 91.667 (92.849)
Test	  accuracy: 77.885 (Err: 0.576 )

Epoch: [47]	 Loss 0.1502 (0.1819)	 acc 92.708 (92.600)
Test	  accuracy: 77.564 (Err: 0.725 )

Epoch: [48]	 Loss 0.1359 (0.1735)	 acc 94.792 (93.271)
Test	  accuracy: 82.692 (Err: 0.473 )

Epoch: [49]	 Loss 0.1755 (0.1729)	 acc 92.708 (93.271)
Test	  accuracy: 75.481 (Err: 0.744 )

Epoch: [50]	 Loss 0.1514 (0.1679)	 acc 94.792 (93.597)
Test	  accuracy: 73.077 (Err: 0.909 )

Epoch: [51]	 Loss 0.0619 (0.1755)	 acc 96.875 (93.328)
Test	  accuracy: 82.532 (Err: 0.448 )

Epoch: [52]	 Loss 0.1268 (0.1713)	 acc 95.833 (93.424)
Test	  accuracy: 78.045 (Err: 0.584 )

Epoch: [53]	 Loss 0.1186 (0.1533)	 acc 94.792 (94.133)
Test	  accuracy: 69.391 (Err: 1.227 )

Epoch: [54]	 Loss 0.3081 (0.1844)	 acc 84.375 (92.734)
Test	  accuracy: 83.494 (Err: 0.430 )

Epoch: [55]	 Loss 0.1407 (0.1666)	 acc 95.833 (93.501)
Test	  accuracy: 75.000 (Err: 0.695 )

Epoch: [56]	 Loss 0.0718 (0.1593)	 acc 98.958 (94.325)
Test	  accuracy: 78.526 (Err: 0.632 )

Epoch: [57]	 Loss 0.1384 (0.1576)	 acc 93.750 (93.769)
Test	  accuracy: 75.160 (Err: 0.822 )

Epoch: [58]	 Loss 0.1004 (0.1603)	 acc 94.792 (93.501)
Test	  accuracy: 83.814 (Err: 0.441 )

Epoch: [59]	 Loss 0.1091 (0.1606)	 acc 95.833 (93.731)
Test	  accuracy: 82.051 (Err: 0.497 )

Epoch: [60]	 Loss 0.1426 (0.1468)	 acc 92.708 (94.479)
Test	  accuracy: 74.519 (Err: 0.769 )

Epoch: [61]	 Loss 0.1362 (0.1659)	 acc 93.750 (93.501)
Test	  accuracy: 76.923 (Err: 0.627 )

Epoch: [62]	 Loss 0.1440 (0.1605)	 acc 96.875 (93.846)
Test	  accuracy: 77.724 (Err: 0.614 )

Epoch: [63]	 Loss 0.1239 (0.1444)	 acc 95.833 (94.306)
Test	  accuracy: 78.846 (Err: 0.632 )

Epoch: [64]	 Loss 0.1090 (0.1520)	 acc 95.833 (93.961)
Test	  accuracy: 78.686 (Err: 0.606 )

Epoch: [65]	 Loss 0.0921 (0.1386)	 acc 97.917 (94.766)
Test	  accuracy: 81.410 (Err: 0.475 )

Epoch: [66]	 Loss 0.1575 (0.1550)	 acc 92.708 (93.692)
Test	  accuracy: 87.500 (Err: 0.379 )

Epoch: [67]	 Loss 0.1263 (0.1714)	 acc 93.750 (93.367)
Test	  accuracy: 77.724 (Err: 0.665 )

Epoch: [68]	 Loss 0.1368 (0.1410)	 acc 95.833 (94.900)
Test	  accuracy: 79.968 (Err: 0.574 )

Epoch: [69]	 Loss 0.2494 (0.1406)	 acc 88.542 (94.421)
Test	  accuracy: 73.077 (Err: 0.873 )

Epoch: [70]	 Loss 0.1584 (0.1429)	 acc 92.708 (94.594)
Test	  accuracy: 76.603 (Err: 0.642 )

Epoch: [71]	 Loss 0.0618 (0.1458)	 acc 97.917 (94.210)
Test	  accuracy: 79.968 (Err: 0.646 )

Epoch: [72]	 Loss 0.1244 (0.1564)	 acc 94.792 (93.903)
Test	  accuracy: 84.455 (Err: 0.390 )

Epoch: [73]	 Loss 0.1238 (0.1460)	 acc 93.750 (94.287)
Test	  accuracy: 74.840 (Err: 0.757 )

Epoch: [74]	 Loss 0.1191 (0.1267)	 acc 93.750 (95.150)
Test	  accuracy: 78.045 (Err: 0.657 )

Epoch: [75]	 Loss 0.1070 (0.1432)	 acc 96.875 (94.402)
Test	  accuracy: 81.731 (Err: 0.506 )

Epoch: [76]	 Loss 0.1233 (0.1540)	 acc 93.750 (94.210)
Test	  accuracy: 78.686 (Err: 0.644 )

Epoch: [77]	 Loss 0.1564 (0.1365)	 acc 91.667 (94.613)
Test	  accuracy: 75.000 (Err: 0.816 )

Epoch: [78]	 Loss 0.0974 (0.1386)	 acc 96.875 (94.440)
Test	  accuracy: 82.692 (Err: 0.472 )

Epoch: [79]	 Loss 0.0875 (0.1346)	 acc 95.833 (94.651)
Test	  accuracy: 83.013 (Err: 0.499 )

Epoch: [80]	 Loss 0.1140 (0.1395)	 acc 92.708 (94.594)
Test	  accuracy: 74.519 (Err: 0.827 )

Epoch: [81]	 Loss 0.1468 (0.1292)	 acc 94.792 (94.843)
Test	  accuracy: 74.359 (Err: 0.812 )

Epoch: [82]	 Loss 0.1374 (0.1336)	 acc 94.792 (94.632)
Test	  accuracy: 75.801 (Err: 0.801 )

Epoch: [83]	 Loss 0.1026 (0.1288)	 acc 96.875 (94.766)
Test	  accuracy: 78.365 (Err: 0.620 )

Epoch: [84]	 Loss 0.1630 (0.1344)	 acc 94.792 (94.824)
Test	  accuracy: 75.641 (Err: 0.775 )

Epoch: [85]	 Loss 0.1457 (0.1492)	 acc 93.750 (94.191)
Test	  accuracy: 82.532 (Err: 0.431 )

Epoch: [86]	 Loss 0.1312 (0.1327)	 acc 94.792 (94.555)
Test	  accuracy: 76.442 (Err: 0.745 )

Epoch: [87]	 Loss 0.0872 (0.1280)	 acc 97.917 (95.226)
Test	  accuracy: 79.808 (Err: 0.637 )

Epoch: [88]	 Loss 0.0952 (0.1294)	 acc 95.833 (94.939)
Test	  accuracy: 75.962 (Err: 0.771 )

Epoch: [89]	 Loss 0.0694 (0.1260)	 acc 96.875 (95.207)
Test	  accuracy: 75.641 (Err: 0.759 )

Epoch: [90]	 Loss 0.1470 (0.1278)	 acc 94.792 (95.073)
Test	  accuracy: 77.885 (Err: 0.683 )

Epoch: [91]	 Loss 0.1363 (0.1313)	 acc 95.833 (94.977)
Test	  accuracy: 74.679 (Err: 0.776 )

Epoch: [92]	 Loss 0.0650 (0.1318)	 acc 98.958 (95.360)
Test	  accuracy: 79.968 (Err: 0.602 )

Epoch: [93]	 Loss 0.2073 (0.1219)	 acc 93.750 (95.284)
Test	  accuracy: 70.513 (Err: 0.971 )

Epoch: [94]	 Loss 0.1015 (0.1338)	 acc 96.875 (94.785)
Test	  accuracy: 83.333 (Err: 0.463 )

Epoch: [95]	 Loss 0.1144 (0.1359)	 acc 96.875 (94.766)
Test	  accuracy: 74.840 (Err: 0.874 )

Epoch: [96]	 Loss 0.1317 (0.1298)	 acc 94.792 (95.150)
Test	  accuracy: 79.487 (Err: 0.522 )

Epoch: [97]	 Loss 0.1163 (0.1269)	 acc 94.792 (94.766)
Test	  accuracy: 78.686 (Err: 0.611 )

Epoch: [98]	 Loss 0.2871 (0.1330)	 acc 85.417 (94.900)
Test	  accuracy: 80.128 (Err: 0.550 )

Epoch: [99]	 Loss 0.1879 (0.1366)	 acc 92.708 (94.670)
Test	  accuracy: 76.282 (Err: 0.700 )

{'best_acc': 87.5, 'test_accs': [62.66025641025641, 63.461538485991646, 64.10256407811092, 69.39102564102564, 66.9871794627263, 76.92307672745142, 80.44871755746695, 75.16025641025641, 80.92948757073817, 81.08974300286708, 80.28846144064879, 80.4487178509052, 79.96794842451047, 80.76923037797977, 80.12820493257962, 80.44871794871794, 83.33333313770784, 79.32692278348483, 81.0897439809946, 79.00641035422301, 82.05128166003105, 82.53205049954929, 79.32692268567207, 82.53205089080028, 79.96794871794872, 80.4487180465307, 79.64743550618489, 81.57051252707457, 81.41025650806917, 80.6089742611616, 79.96794852232321, 78.36538481101012, 80.92948698386168, 82.8525635156876, 85.41666607979016, 82.53205118423853, 85.09615384615384, 71.9551282051282, 83.49358993921524, 74.03846173408704, 76.76282070844601, 71.31410256410257, 78.52564112345378, 77.88461509117714, 78.04487159924629, 83.97435858310797, 77.88461509117714, 77.5641022706643, 82.69230779012044, 75.48076913295648, 73.07692317473582, 82.53205118423853, 78.04487169705905, 69.39102554321289, 83.49358954796425, 74.99999990218726, 78.52564092782828, 75.1602560190054, 83.81410256410257, 82.05128185565655, 74.51923067141801, 76.92307672745142, 77.72435858310797, 78.8461537483411, 78.68589733808469, 81.41025611681816, 87.5, 77.72435907217172, 79.96794842451047, 73.07692293020395, 76.6025642981896, 79.96794881576147, 84.4551280095027, 74.83974319849258, 78.04487189268454, 81.73076903514372, 78.68589753371019, 75.00000009781274, 82.69230749668219, 83.01282061063327, 74.51923037797977, 74.35897435897436, 75.80128166003105, 78.36538422413362, 75.64102554321289, 82.53205089080028, 76.44230749668219, 79.8076923076923, 75.9615381681002, 75.64102524977464, 77.88461538461539, 74.67948737511269, 79.96794832669772, 70.51282046391414, 83.33333352895883, 74.83974349193083, 79.48717948717949, 78.68589763152293, 80.12820532383063, 76.28205128205128], 'test_losses': [0.6817407210667928, 0.6743788902576153, 0.692025132668324, 0.6673795504447742, 0.6076480914384891, 0.4507029897127396, 0.4253232983442453, 0.6222721063173734, 0.4509162948681758, 0.4330005340087108, 0.4913779329030942, 0.5234266580679477, 0.527158776919047, 0.5440870294204125, 0.5137168902617234, 0.4705532819796831, 0.3855913969186636, 0.5909811334732251, 0.5502783717253269, 0.5922122108630645, 0.5076422538512793, 0.4496393112035898, 0.7240084333297534, 0.454995286770356, 0.6418612331916125, 0.5698229571183523, 0.5362792290174044, 0.526578542513725, 0.5497788832737849, 0.6153333836641067, 0.6343432573171762, 0.6805795629819235, 0.5872344305882087, 0.4344843870554215, 0.37893758675990963, 0.4609707609201089, 0.3691317698894403, 0.9072246475097461, 0.4106106666418222, 0.9001462142436932, 0.6825509285315489, 0.7349833253866587, 0.6197762573376681, 0.6505480798391196, 0.5927412097270672, 0.4689408494875981, 0.5760024304573352, 0.7247731383794394, 0.47309760863964373, 0.7443275524255557, 0.9085606029018377, 0.4482282797495524, 0.5839875485652533, 1.2268510130353463, 0.42999972288425153, 0.6952009132275214, 0.6323687296647292, 0.8216026753951342, 0.44075427605555606, 0.4972843642418201, 0.7685900395497297, 0.6265719422163107, 0.6138153645472649, 0.631951263317695, 0.6058587042184976, 0.47463655930299026, 0.3791981339454651, 0.6652873628414594, 0.5741874506840339, 0.8727690096084888, 0.6424589932729037, 0.6461394681380346, 0.39009765019783604, 0.7573500928970484, 0.6572600454092026, 0.5064811538427304, 0.6441929122576346, 0.8159914975747083, 0.47165891298880946, 0.49947998309746766, 0.8269881224020933, 0.8118199037435727, 0.8007610363838, 0.6203812376046792, 0.7746652158407065, 0.43065867668543106, 0.7447264752326868, 0.6372769180780802, 0.7710723972473389, 0.7586917311717303, 0.6825275489917169, 0.7763879199822744, 0.6016545097033182, 0.9714886662669671, 0.4633483733886328, 0.8740182148340421, 0.5217757981557113, 0.6107563575108846, 0.550356910778926, 0.6999988150902283], 'train_acss': [73.12116555055958, 74.65490797546012, 76.66794473846997, 78.02914110429448, 80.27223926380368, 82.22776064258412, 84.31748456896449, 85.31441713110802, 86.1963190184049, 86.54141099613868, 84.8351226993865, 87.40414110429448, 88.51610429447852, 88.28604289797917, 87.65337423312883, 89.16794478527608, 89.66641099613868, 89.2638035873694, 88.78450910884179, 89.64723921699758, 89.66641099613868, 89.99233119473135, 90.08819013724298, 90.24156432356571, 90.83588947693994, 90.22239254442461, 90.2032208588957, 90.47162567325897, 90.75920236037553, 90.81671779141104, 91.33435582822086, 91.37269933969696, 91.73696314337795, 91.1042943849154, 91.60276068939022, 91.37269929289087, 91.02760731515708, 91.58358886344301, 91.67944780595464, 91.31518395546755, 92.36963185503438, 92.81058282208589, 92.75306743785647, 92.10122690025283, 92.94478518246142, 92.48466252988102, 92.84892633356199, 92.59969315792154, 93.27070547466629, 93.2707054278602, 93.59662572006506, 93.3282208588957, 93.42407966098902, 94.13343553601598, 92.73389570552148, 93.50076677755344, 94.32515328062092, 93.7691717791411, 93.50076682435954, 93.7308281272467, 94.47852751374975, 93.50076687116564, 93.84585889570552, 94.30598150147982, 93.96088947693994, 94.76610424767243, 93.69248456896449, 93.36656441717791, 94.90030665485405, 94.42101222313255, 94.59355818859639, 94.2101226525804, 93.90337418632274, 94.28680981595092, 95.14953987730061, 94.40184049079754, 94.2101226993865, 94.61273001454359, 94.44018404907976, 94.6510735260197, 94.59355818859639, 94.84279136423685, 94.63190179368469, 94.76610429447852, 94.82361958509574, 94.1909509202454, 94.55521467712028, 95.22622694705893, 94.93865021313627, 95.20705521472392, 95.07285271393009, 94.97699377141848, 95.36042935424055, 95.28374233128834, 94.78527607361963, 94.76610429447852, 95.14953983049452, 94.76610424767243, 94.90030670166016, 94.6702453051608], 'train_losses': [0.5435067166580013, 0.472973321478791, 0.4574895415203703, 0.44215692241499027, 0.4082013476480004, 0.3799816406577643, 0.33697097158870815, 0.32849372128036125, 0.30731371858734297, 0.29622715409913675, 0.3290926570906961, 0.28528422757160443, 0.26449067077022387, 0.2655790863227259, 0.276477669554254, 0.2573687205651055, 0.24435672419934185, 0.2533713741902193, 0.2580436582397098, 0.2471144576928367, 0.24105738578764208, 0.2389835162213975, 0.23966961696835384, 0.23924873623014228, 0.22106882006478457, 0.23630836391193002, 0.23661195125682222, 0.22719348034610046, 0.22163382940497134, 0.21456401507181624, 0.208910130695697, 0.2093731401705303, 0.20562267138914098, 0.22362110558097348, 0.20571280266243988, 0.21347001812026545, 0.21997319929804537, 0.20505964815433772, 0.19563168159291788, 0.21307645747266665, 0.19154657642899847, 0.18340119820065293, 0.18392710424274022, 0.1930501780078455, 0.1817530556324801, 0.1883065283664165, 0.1832464855689944, 0.18189277129670595, 0.17349273121795772, 0.17285102925783286, 0.16785343430159283, 0.17547198926652868, 0.17130951281705517, 0.15326096167776482, 0.18436709247483798, 0.16658468215377784, 0.1592692184759064, 0.1575971138806431, 0.16028457953154676, 0.1605702808663889, 0.1467793432115777, 0.1659252828249902, 0.16048238970385006, 0.1443569821082741, 0.15200372767046186, 0.13859110028465832, 0.15503682378976624, 0.1714056892080541, 0.14098743884110013, 0.1406204112651158, 0.14288115729949225, 0.14579459204355633, 0.15637265813131274, 0.1460114911290034, 0.12666121632592078, 0.14321026667678283, 0.1539601908048238, 0.1364549106798289, 0.13862209684826846, 0.13463660448423923, 0.13948074632261429, 0.12924701158254426, 0.13360513655320266, 0.1287857276256099, 0.1344131116121093, 0.1492148371744741, 0.13266460570089655, 0.1280012890879362, 0.1293727474435707, 0.12601149717540097, 0.12777034782924535, 0.1312641363385265, 0.13176802591494988, 0.12186531311163873, 0.13377242841603565, 0.13593875827050647, 0.12981329373786785, 0.12685574833417962, 0.13303134869212754, 0.13661769416434633]}
2022-01-01 21:31:38.460387


